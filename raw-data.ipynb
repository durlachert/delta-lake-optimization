{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2fa7be1-db62-4009-a7d9-a045b32f4d11",
   "metadata": {},
   "source": [
    "# Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73e9a156-06cb-43cc-947e-2786f67c334c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/05 19:06:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RawData\") \\\n",
    "    .master(\"spark://192.168.0.144:7077\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://192.168.0.144:9083\") \\\n",
    "    .config(\"spark.hadoop.javax.jdo.option.ConnectionURL\", \"jdbc:mysql://192.168.0.144:3306/metastore_db\") \\\n",
    "    .config(\"spark.hadoop.javax.jdo.option.ConnectionDriverName\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .config(\"spark.hadoop.javax.jdo.option.ConnectionUserName\", \"lh\") \\\n",
    "    .config(\"spark.hadoop.javax.jdo.option.ConnectionPassword\", os.getenv('MYSQL', 'Default_Value')) \\\n",
    "    .config(\"spark.jars\", \"/usr/local/spark/jars/delta-storage-3.2.0.jar,/usr/local/spark/jars/delta-spark_2.12-3.2.0.jar\")    \\\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.HDFSLogStore\") \\\n",
    "    .config(\"spark.executor.memory\", \"9g\") \\\n",
    "    .config(\"spark.executor.cores\", \"3\") \\\n",
    "    .config(\"spark.driver.memory\", \"19g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://192.168.0.144:9000\") \\\n",
    "    .config(\"spark.databricks.delta.clusteredTable.enableClusteringTablePreview\", \"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7586a575-7aa8-49e7-b074-d61e6ec37217",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb65454d-33a7-4ecb-b700-8fa84d9f283a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.sql.warehouse.dir', 'file:/home/lh/spark-warehouse'),\n",
       " ('spark.app.startTime', '1722863728957'),\n",
       " ('spark.hadoop.javax.jdo.option.ConnectionUserName', 'lh'),\n",
       " ('spark.app.id', 'app-20240805131532-0000'),\n",
       " ('spark.jars',\n",
       "  '/usr/local/spark/jars/delta-storage-3.2.0.jar,/usr/local/spark/jars/delta-spark_2.12-3.2.0.jar'),\n",
       " ('spark.hadoop.javax.jdo.option.ConnectionURL',\n",
       "  'jdbc:mysql://192.168.0.144:3306/metastore_db'),\n",
       " ('spark.app.submitTime', '1722863728538'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///usr/local/spark/jars/delta-storage-3.2.0.jar,file:///usr/local/spark/jars/delta-spark_2.12-3.2.0.jar'),\n",
       " ('spark.delta.logStore.class',\n",
       "  'org.apache.spark.sql.delta.storage.HDFSLogStore'),\n",
       " ('spark.driver.memory', '19g'),\n",
       " ('spark.hadoop.javax.jdo.option.ConnectionPassword', 'Default_Value'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.executor.cores', '3'),\n",
       " ('spark.driver.port', '41161'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'MyApp04'),\n",
       " ('spark.driver.host', 'lh'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.executor.memory', '9g'),\n",
       " ('spark.hadoop.hive.metastore.uris', 'thrift://192.168.0.144:9083'),\n",
       " ('spark.databricks.delta.clusteredTable.enableClusteringTablePreview',\n",
       "  'true'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.initial.jar.urls',\n",
       "  'spark://lh:41161/jars/delta-storage-3.2.0.jar,spark://lh:41161/jars/delta-spark_2.12-3.2.0.jar'),\n",
       " ('spark.hadoop.fs.defaultFS', 'hdfs://192.168.0.144:9000'),\n",
       " ('spark.hadoop.javax.jdo.option.ConnectionDriverName',\n",
       "  'com.mysql.cj.jdbc.Driver'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.driver.maxResultSize', '2g'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.master', 'spark://192.168.0.144:7077'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.catalog.spark_catalog',\n",
       "  'org.apache.spark.sql.delta.catalog.DeltaCatalog')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5094ee55-7606-4c2a-ae37-b8bea30c6bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "db='raw_data'\n",
    "delta_db_path = 'hdfs://192.168.0.144:9000/datalake/raw_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7de54808-41cd-4a6b-a842-99aff2883433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spark.sql(f\"drop DATABASE {db} CASCADE\")\n",
    "#spark.sql(f\"CREATE DATABASE {db} LOCATION '{delta_db_path}'\")\n",
    "spark.sql(f\"use database {db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ce524e2-73b1-4ac9-b52e-4c8bd5d2ab69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Parallelism: 15\n",
      "Shuffle Partitions: 200\n"
     ]
    }
   ],
   "source": [
    "# Check config\n",
    "default_parallelism = spark.sparkContext.defaultParallelism\n",
    "print(f\"Default Parallelism: {default_parallelism}\")\n",
    "#spark.conf.set(\"spark.default.parallelism\", \"100\")\n",
    "shuffle_partitions = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "print(f\"Shuffle Partitions: {shuffle_partitions}\")\n",
    "#spark.conf.set(\"spark.sql.shuffle.partitions\", \"100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d96a186-45d4-4fb5-a690-ffe2461a4c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/05 13:21:41 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- App ID: string (nullable = true)\n",
      " |-- App Name: string (nullable = true)\n",
      " |-- Block Manager ID: struct (nullable = true)\n",
      " |    |-- Executor ID: string (nullable = true)\n",
      " |    |-- Host: string (nullable = true)\n",
      " |    |-- Port: long (nullable = true)\n",
      " |-- Classpath Entries: struct (nullable = true)\n",
      " |    |-- /usr/local/spark/conf/: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/HikariCP-2.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/JLargeArrays-1.5.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/JTransforms-3.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/RoaringBitmap-0.9.45.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/ST4-4.0.4.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/activation-1.1.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/aircompressor-0.26.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/algebra_2.12-2.0.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/annotations-17.0.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/antlr-runtime-3.5.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/antlr4-runtime-4.9.3.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/aopalliance-repackaged-2.6.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/arpack-3.0.3.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/arpack_combined_all-0.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/arrow-format-12.0.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/arrow-memory-core-12.0.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/arrow-memory-netty-12.0.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/arrow-vector-12.0.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/audience-annotations-0.5.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/avro-1.11.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/avro-ipc-1.11.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/avro-mapred-1.11.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/blas-3.0.3.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/bonecp-0.8.0.RELEASE.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/breeze-macros_2.12-2.1.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/breeze_2.12-2.1.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/cats-kernel_2.12-2.1.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/chill-java-0.10.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/chill_2.12-0.10.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/commons-cli-1.5.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/commons-codec-1.16.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/commons-collections-3.2.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/commons-collections4-4.4.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/commons-compiler-3.1.9.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/commons-compress-1.23.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/commons-crypto-1.1.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/commons-dbcp-1.4.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/commons-io-2.13.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/commons-lang-2.6.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/commons-lang3-3.12.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/commons-logging-1.1.3.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/commons-math3-3.6.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/commons-pool-1.5.4.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/commons-text-1.10.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/compress-lzf-1.1.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/curator-client-2.13.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/curator-framework-2.13.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/curator-recipes-2.13.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/datanucleus-api-jdo-4.2.4.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/datanucleus-core-4.1.17.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/datanucleus-rdbms-4.1.19.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/datasketches-java-3.3.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/datasketches-memory-2.1.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/delta-spark_2.12-3.2.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/delta-storage-3.2.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/derby-10.14.2.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/flatbuffers-java-1.12.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/gson-2.2.4.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/guava-14.0.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/hadoop-client-api-3.3.4.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/hadoop-client-runtime-3.3.4.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/hadoop-shaded-guava-1.1.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/hadoop-yarn-server-web-proxy-3.3.4.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/hive-beeline-2.3.9.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/hive-cli-2.3.9.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/hive-common-2.3.9.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/hive-exec-2.3.9-core.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/hive-jdbc-2.3.9.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/hive-llap-common-2.3.9.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/hive-metastore-2.3.9.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/hive-serde-2.3.9.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/hive-service-rpc-3.1.3.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/hive-shims-0.23-2.3.9.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/hive-shims-2.3.9.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/hive-shims-common-2.3.9.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/hive-shims-scheduler-2.3.9.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/hive-storage-api-2.8.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/hk2-api-2.6.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/hk2-locator-2.6.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/hk2-utils-2.6.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/httpclient-4.5.14.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/httpcore-4.4.16.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/istack-commons-runtime-3.0.8.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/ivy-2.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jackson-annotations-2.15.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jackson-core-2.15.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jackson-core-asl-1.9.13.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jackson-databind-2.15.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jackson-dataformat-yaml-2.15.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jackson-datatype-jsr310-2.15.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jackson-mapper-asl-1.9.13.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jackson-module-scala_2.12-2.15.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jakarta.annotation-api-1.3.5.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jakarta.inject-2.6.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jakarta.servlet-api-4.0.3.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jakarta.validation-api-2.0.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jakarta.ws.rs-api-2.1.6.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jakarta.xml.bind-api-2.3.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/janino-3.1.9.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/javassist-3.29.2-GA.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/javax.jdo-3.2.0-m3.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/javolution-5.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jaxb-runtime-2.3.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jcl-over-slf4j-2.0.7.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jdo-api-3.0.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jersey-client-2.40.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jersey-common-2.40.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jersey-container-servlet-2.40.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jersey-container-servlet-core-2.40.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jersey-hk2-2.40.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jersey-server-2.40.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jline-2.14.6.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jmx_prometheus_javaagent-1.0.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/joda-time-2.12.5.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jodd-core-3.5.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jpam-1.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/json-1.8.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/json4s-ast_2.12-3.7.0-M11.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/json4s-core_2.12-3.7.0-M11.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/json4s-jackson_2.12-3.7.0-M11.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/json4s-scalap_2.12-3.7.0-M11.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jsr305-3.0.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jta-1.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/jul-to-slf4j-2.0.7.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kryo-shaded-4.0.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-client-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-client-api-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-httpclient-okhttp-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-model-admissionregistration-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-model-apiextensions-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-model-apps-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-model-autoscaling-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-model-batch-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-model-certificates-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-model-common-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-model-coordination-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-model-core-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-model-discovery-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-model-events-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-model-extensions-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-model-flowcontrol-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-model-gatewayapi-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-model-metrics-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-model-networking-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-model-node-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-model-policy-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-model-rbac-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-model-resource-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-model-scheduling-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/kubernetes-model-storageclass-6.7.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/lapack-3.0.3.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/leveldbjni-all-1.8.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/libfb303-0.9.3.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/libthrift-0.12.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/log4j-1.2-api-2.20.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/log4j-api-2.20.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/log4j-core-2.20.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/log4j-slf4j2-impl-2.20.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/logging-interceptor-3.12.12.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/lz4-java-1.8.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/mesos-1.4.3-shaded-protobuf.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/metrics-core-4.2.19.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/metrics-graphite-4.2.19.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/metrics-jmx-4.2.19.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/metrics-json-4.2.19.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/metrics-jvm-4.2.19.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/minlog-1.3.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/netty-all-4.1.96.Final.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/netty-buffer-4.1.96.Final.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/netty-codec-4.1.96.Final.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/netty-codec-http-4.1.96.Final.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/netty-codec-http2-4.1.96.Final.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/netty-codec-socks-4.1.96.Final.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/netty-common-4.1.96.Final.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/netty-handler-4.1.96.Final.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/netty-handler-proxy-4.1.96.Final.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/netty-resolver-4.1.96.Final.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/netty-transport-4.1.96.Final.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/netty-transport-classes-epoll-4.1.96.Final.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/netty-transport-classes-kqueue-4.1.96.Final.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/netty-transport-native-epoll-4.1.96.Final-linux-aarch_64.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/netty-transport-native-epoll-4.1.96.Final-linux-x86_64.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/netty-transport-native-kqueue-4.1.96.Final-osx-aarch_64.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/netty-transport-native-kqueue-4.1.96.Final-osx-x86_64.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/netty-transport-native-unix-common-4.1.96.Final.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/objenesis-3.3.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/okhttp-3.12.12.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/okio-1.15.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/opencsv-2.3.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/orc-core-1.9.2-shaded-protobuf.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/orc-mapreduce-1.9.2-shaded-protobuf.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/orc-shims-1.9.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/oro-2.0.8.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/osgi-resource-locator-1.0.3.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/paranamer-2.8.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/parquet-column-1.13.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/parquet-common-1.13.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/parquet-encoding-1.13.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/parquet-format-structures-1.13.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/parquet-hadoop-1.13.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/parquet-jackson-1.13.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/pickle-1.3.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/py4j-0.10.9.7.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/rocksdbjni-8.3.2.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/scala-collection-compat_2.12-2.7.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/scala-compiler-2.12.18.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/scala-library-2.12.18.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/scala-parser-combinators_2.12-2.3.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/scala-reflect-2.12.18.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/scala-xml_2.12-2.1.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/shims-0.9.45.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/slf4j-api-2.0.7.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/snakeyaml-2.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/snakeyaml-engine-2.6.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/snappy-java-1.1.10.3.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spark-catalyst_2.12-3.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spark-common-utils_2.12-3.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spark-core_2.12-3.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spark-graphx_2.12-3.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spark-hive-thriftserver_2.12-3.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spark-hive_2.12-3.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spark-kubernetes_2.12-3.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spark-kvstore_2.12-3.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spark-launcher_2.12-3.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spark-mesos_2.12-3.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spark-mllib-local_2.12-3.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spark-mllib_2.12-3.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spark-network-common_2.12-3.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spark-network-shuffle_2.12-3.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spark-repl_2.12-3.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spark-sketch_2.12-3.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spark-sql-api_2.12-3.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spark-sql_2.12-3.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spark-streaming_2.12-3.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spark-tags_2.12-3.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spark-unsafe_2.12-3.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spark-yarn_2.12-3.5.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spire-macros_2.12-0.17.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spire-platform_2.12-0.17.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spire-util_2.12-0.17.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/spire_2.12-0.17.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/stax-api-1.0.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/stream-2.9.6.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/super-csv-2.2.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/threeten-extra-1.7.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/tink-1.9.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/transaction-api-1.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/univocity-parsers-2.9.1.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/xbean-asm9-shaded-4.23.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/xz-1.9.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/zjsonpatch-0.3.0.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/zookeeper-3.6.3.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/zookeeper-jute-3.6.3.jar: string (nullable = true)\n",
      " |    |-- /usr/local/spark/jars/zstd-jni-1.5.5-4.jar: string (nullable = true)\n",
      " |    |-- spark://lh:39065/jars/delta-spark_2.12-3.2.0.jar: string (nullable = true)\n",
      " |    |-- spark://lh:39065/jars/delta-storage-3.2.0.jar: string (nullable = true)\n",
      " |    |-- spark://lh:44045/jars/delta-spark_2.12-3.2.0.jar: string (nullable = true)\n",
      " |    |-- spark://lh:44045/jars/delta-storage-3.2.0.jar: string (nullable = true)\n",
      " |    |-- spark://lh:44193/jars/delta-spark_2.12-3.2.0.jar: string (nullable = true)\n",
      " |    |-- spark://lh:44193/jars/delta-storage-3.2.0.jar: string (nullable = true)\n",
      " |-- Completion Time: long (nullable = true)\n",
      " |-- Event: string (nullable = true)\n",
      " |-- Executor ID: string (nullable = true)\n",
      " |-- Executor Info: struct (nullable = true)\n",
      " |    |-- Host: string (nullable = true)\n",
      " |    |-- Log Urls: struct (nullable = true)\n",
      " |    |    |-- stderr: string (nullable = true)\n",
      " |    |    |-- stdout: string (nullable = true)\n",
      " |    |-- Registration Time: long (nullable = true)\n",
      " |    |-- Resource Profile Id: long (nullable = true)\n",
      " |    |-- Total Cores: long (nullable = true)\n",
      " |-- Executor Resource Requests: struct (nullable = true)\n",
      " |    |-- cores: struct (nullable = true)\n",
      " |    |    |-- Amount: long (nullable = true)\n",
      " |    |    |-- Discovery Script: string (nullable = true)\n",
      " |    |    |-- Resource Name: string (nullable = true)\n",
      " |    |    |-- Vendor: string (nullable = true)\n",
      " |    |-- memory: struct (nullable = true)\n",
      " |    |    |-- Amount: long (nullable = true)\n",
      " |    |    |-- Discovery Script: string (nullable = true)\n",
      " |    |    |-- Resource Name: string (nullable = true)\n",
      " |    |    |-- Vendor: string (nullable = true)\n",
      " |    |-- offHeap: struct (nullable = true)\n",
      " |    |    |-- Amount: long (nullable = true)\n",
      " |    |    |-- Discovery Script: string (nullable = true)\n",
      " |    |    |-- Resource Name: string (nullable = true)\n",
      " |    |    |-- Vendor: string (nullable = true)\n",
      " |-- Hadoop Properties: struct (nullable = true)\n",
      " |    |-- adl.feature.ownerandgroup.enableupn: string (nullable = true)\n",
      " |    |-- adl.http.timeout: string (nullable = true)\n",
      " |    |-- dfs.client.ignore.namenode.default.kms.uri: string (nullable = true)\n",
      " |    |-- dfs.ha.fencing.ssh.connect-timeout: string (nullable = true)\n",
      " |    |-- file.blocksize: string (nullable = true)\n",
      " |    |-- file.bytes-per-checksum: string (nullable = true)\n",
      " |    |-- file.client-write-packet-size: string (nullable = true)\n",
      " |    |-- file.replication: string (nullable = true)\n",
      " |    |-- file.stream-buffer-size: string (nullable = true)\n",
      " |    |-- fs.AbstractFileSystem.abfs.impl: string (nullable = true)\n",
      " |    |-- fs.AbstractFileSystem.abfss.impl: string (nullable = true)\n",
      " |    |-- fs.AbstractFileSystem.adl.impl: string (nullable = true)\n",
      " |    |-- fs.AbstractFileSystem.file.impl: string (nullable = true)\n",
      " |    |-- fs.AbstractFileSystem.ftp.impl: string (nullable = true)\n",
      " |    |-- fs.AbstractFileSystem.gs.impl: string (nullable = true)\n",
      " |    |-- fs.AbstractFileSystem.har.impl: string (nullable = true)\n",
      " |    |-- fs.AbstractFileSystem.hdfs.impl: string (nullable = true)\n",
      " |    |-- fs.AbstractFileSystem.s3a.impl: string (nullable = true)\n",
      " |    |-- fs.AbstractFileSystem.swebhdfs.impl: string (nullable = true)\n",
      " |    |-- fs.AbstractFileSystem.viewfs.impl: string (nullable = true)\n",
      " |    |-- fs.AbstractFileSystem.wasb.impl: string (nullable = true)\n",
      " |    |-- fs.AbstractFileSystem.wasbs.impl: string (nullable = true)\n",
      " |    |-- fs.AbstractFileSystem.webhdfs.impl: string (nullable = true)\n",
      " |    |-- fs.abfs.impl: string (nullable = true)\n",
      " |    |-- fs.abfss.impl: string (nullable = true)\n",
      " |    |-- fs.adl.impl: string (nullable = true)\n",
      " |    |-- fs.adl.oauth2.access.token.provider.type: string (nullable = true)\n",
      " |    |-- fs.automatic.close: string (nullable = true)\n",
      " |    |-- fs.azure.authorization: string (nullable = true)\n",
      " |    |-- fs.azure.authorization.caching.enable: string (nullable = true)\n",
      " |    |-- fs.azure.buffer.dir: string (nullable = true)\n",
      " |    |-- fs.azure.local.sas.key.mode: string (nullable = true)\n",
      " |    |-- fs.azure.sas.expiry.period: string (nullable = true)\n",
      " |    |-- fs.azure.saskey.usecontainersaskeyforallaccess: string (nullable = true)\n",
      " |    |-- fs.azure.secure.mode: string (nullable = true)\n",
      " |    |-- fs.azure.user.agent.prefix: string (nullable = true)\n",
      " |    |-- fs.client.resolve.remote.symlinks: string (nullable = true)\n",
      " |    |-- fs.client.resolve.topology.enabled: string (nullable = true)\n",
      " |    |-- fs.defaultFS: string (nullable = true)\n",
      " |    |-- fs.df.interval: string (nullable = true)\n",
      " |    |-- fs.du.interval: string (nullable = true)\n",
      " |    |-- fs.ftp.data.connection.mode: string (nullable = true)\n",
      " |    |-- fs.ftp.host: string (nullable = true)\n",
      " |    |-- fs.ftp.host.port: string (nullable = true)\n",
      " |    |-- fs.ftp.impl: string (nullable = true)\n",
      " |    |-- fs.ftp.timeout: string (nullable = true)\n",
      " |    |-- fs.ftp.transfer.mode: string (nullable = true)\n",
      " |    |-- fs.getspaceused.jitterMillis: string (nullable = true)\n",
      " |    |-- fs.har.impl.disable.cache: string (nullable = true)\n",
      " |    |-- fs.permissions.umask-mode: string (nullable = true)\n",
      " |    |-- fs.s3a.accesspoint.required: string (nullable = true)\n",
      " |    |-- fs.s3a.assumed.role.credentials.provider: string (nullable = true)\n",
      " |    |-- fs.s3a.assumed.role.session.duration: string (nullable = true)\n",
      " |    |-- fs.s3a.attempts.maximum: string (nullable = true)\n",
      " |    |-- fs.s3a.aws.credentials.provider: string (nullable = true)\n",
      " |    |-- fs.s3a.block.size: string (nullable = true)\n",
      " |    |-- fs.s3a.buffer.dir: string (nullable = true)\n",
      " |    |-- fs.s3a.change.detection.mode: string (nullable = true)\n",
      " |    |-- fs.s3a.change.detection.source: string (nullable = true)\n",
      " |    |-- fs.s3a.change.detection.version.required: string (nullable = true)\n",
      " |    |-- fs.s3a.committer.abort.pending.uploads: string (nullable = true)\n",
      " |    |-- fs.s3a.committer.magic.enabled: string (nullable = true)\n",
      " |    |-- fs.s3a.committer.name: string (nullable = true)\n",
      " |    |-- fs.s3a.committer.staging.conflict-mode: string (nullable = true)\n",
      " |    |-- fs.s3a.committer.staging.tmp.path: string (nullable = true)\n",
      " |    |-- fs.s3a.committer.staging.unique-filenames: string (nullable = true)\n",
      " |    |-- fs.s3a.committer.threads: string (nullable = true)\n",
      " |    |-- fs.s3a.connection.establish.timeout: string (nullable = true)\n",
      " |    |-- fs.s3a.connection.maximum: string (nullable = true)\n",
      " |    |-- fs.s3a.connection.request.timeout: string (nullable = true)\n",
      " |    |-- fs.s3a.connection.ssl.enabled: string (nullable = true)\n",
      " |    |-- fs.s3a.connection.timeout: string (nullable = true)\n",
      " |    |-- fs.s3a.downgrade.syncable.exceptions: string (nullable = true)\n",
      " |    |-- fs.s3a.endpoint: string (nullable = true)\n",
      " |    |-- fs.s3a.etag.checksum.enabled: string (nullable = true)\n",
      " |    |-- fs.s3a.executor.capacity: string (nullable = true)\n",
      " |    |-- fs.s3a.fast.upload.active.blocks: string (nullable = true)\n",
      " |    |-- fs.s3a.fast.upload.buffer: string (nullable = true)\n",
      " |    |-- fs.s3a.impl: string (nullable = true)\n",
      " |    |-- fs.s3a.list.version: string (nullable = true)\n",
      " |    |-- fs.s3a.max.total.tasks: string (nullable = true)\n",
      " |    |-- fs.s3a.metadatastore.authoritative: string (nullable = true)\n",
      " |    |-- fs.s3a.metadatastore.fail.on.write.error: string (nullable = true)\n",
      " |    |-- fs.s3a.metadatastore.impl: string (nullable = true)\n",
      " |    |-- fs.s3a.metadatastore.metadata.ttl: string (nullable = true)\n",
      " |    |-- fs.s3a.multiobjectdelete.enable: string (nullable = true)\n",
      " |    |-- fs.s3a.multipart.purge: string (nullable = true)\n",
      " |    |-- fs.s3a.multipart.purge.age: string (nullable = true)\n",
      " |    |-- fs.s3a.multipart.size: string (nullable = true)\n",
      " |    |-- fs.s3a.multipart.threshold: string (nullable = true)\n",
      " |    |-- fs.s3a.paging.maximum: string (nullable = true)\n",
      " |    |-- fs.s3a.path.style.access: string (nullable = true)\n",
      " |    |-- fs.s3a.readahead.range: string (nullable = true)\n",
      " |    |-- fs.s3a.retry.interval: string (nullable = true)\n",
      " |    |-- fs.s3a.retry.limit: string (nullable = true)\n",
      " |    |-- fs.s3a.retry.throttle.interval: string (nullable = true)\n",
      " |    |-- fs.s3a.retry.throttle.limit: string (nullable = true)\n",
      " |    |-- fs.s3a.s3guard.cli.prune.age: string (nullable = true)\n",
      " |    |-- fs.s3a.s3guard.consistency.retry.interval: string (nullable = true)\n",
      " |    |-- fs.s3a.s3guard.consistency.retry.limit: string (nullable = true)\n",
      " |    |-- fs.s3a.s3guard.ddb.background.sleep: string (nullable = true)\n",
      " |    |-- fs.s3a.s3guard.ddb.max.retries: string (nullable = true)\n",
      " |    |-- fs.s3a.s3guard.ddb.table.capacity.read: string (nullable = true)\n",
      " |    |-- fs.s3a.s3guard.ddb.table.capacity.write: string (nullable = true)\n",
      " |    |-- fs.s3a.s3guard.ddb.table.create: string (nullable = true)\n",
      " |    |-- fs.s3a.s3guard.ddb.table.sse.enabled: string (nullable = true)\n",
      " |    |-- fs.s3a.s3guard.ddb.throttle.retry.interval: string (nullable = true)\n",
      " |    |-- fs.s3a.select.enabled: string (nullable = true)\n",
      " |    |-- fs.s3a.select.errors.include.sql: string (nullable = true)\n",
      " |    |-- fs.s3a.select.input.compression: string (nullable = true)\n",
      " |    |-- fs.s3a.select.input.csv.comment.marker: string (nullable = true)\n",
      " |    |-- fs.s3a.select.input.csv.field.delimiter: string (nullable = true)\n",
      " |    |-- fs.s3a.select.input.csv.header: string (nullable = true)\n",
      " |    |-- fs.s3a.select.input.csv.quote.character: string (nullable = true)\n",
      " |    |-- fs.s3a.select.input.csv.quote.escape.character: string (nullable = true)\n",
      " |    |-- fs.s3a.select.input.csv.record.delimiter: string (nullable = true)\n",
      " |    |-- fs.s3a.select.output.csv.field.delimiter: string (nullable = true)\n",
      " |    |-- fs.s3a.select.output.csv.quote.character: string (nullable = true)\n",
      " |    |-- fs.s3a.select.output.csv.quote.escape.character: string (nullable = true)\n",
      " |    |-- fs.s3a.select.output.csv.quote.fields: string (nullable = true)\n",
      " |    |-- fs.s3a.select.output.csv.record.delimiter: string (nullable = true)\n",
      " |    |-- fs.s3a.socket.recv.buffer: string (nullable = true)\n",
      " |    |-- fs.s3a.socket.send.buffer: string (nullable = true)\n",
      " |    |-- fs.s3a.ssl.channel.mode: string (nullable = true)\n",
      " |    |-- fs.s3a.threads.keepalivetime: string (nullable = true)\n",
      " |    |-- fs.s3a.threads.max: string (nullable = true)\n",
      " |    |-- fs.swift.impl: string (nullable = true)\n",
      " |    |-- fs.trash.checkpoint.interval: string (nullable = true)\n",
      " |    |-- fs.trash.interval: string (nullable = true)\n",
      " |    |-- fs.viewfs.overload.scheme.target.abfs.impl: string (nullable = true)\n",
      " |    |-- fs.viewfs.overload.scheme.target.abfss.impl: string (nullable = true)\n",
      " |    |-- fs.viewfs.overload.scheme.target.file.impl: string (nullable = true)\n",
      " |    |-- fs.viewfs.overload.scheme.target.ftp.impl: string (nullable = true)\n",
      " |    |-- fs.viewfs.overload.scheme.target.gs.impl: string (nullable = true)\n",
      " |    |-- fs.viewfs.overload.scheme.target.hdfs.impl: string (nullable = true)\n",
      " |    |-- fs.viewfs.overload.scheme.target.http.impl: string (nullable = true)\n",
      " |    |-- fs.viewfs.overload.scheme.target.https.impl: string (nullable = true)\n",
      " |    |-- fs.viewfs.overload.scheme.target.o3fs.impl: string (nullable = true)\n",
      " |    |-- fs.viewfs.overload.scheme.target.ofs.impl: string (nullable = true)\n",
      " |    |-- fs.viewfs.overload.scheme.target.oss.impl: string (nullable = true)\n",
      " |    |-- fs.viewfs.overload.scheme.target.s3a.impl: string (nullable = true)\n",
      " |    |-- fs.viewfs.overload.scheme.target.swebhdfs.impl: string (nullable = true)\n",
      " |    |-- fs.viewfs.overload.scheme.target.swift.impl: string (nullable = true)\n",
      " |    |-- fs.viewfs.overload.scheme.target.wasb.impl: string (nullable = true)\n",
      " |    |-- fs.viewfs.overload.scheme.target.webhdfs.impl: string (nullable = true)\n",
      " |    |-- fs.viewfs.rename.strategy: string (nullable = true)\n",
      " |    |-- fs.wasb.impl: string (nullable = true)\n",
      " |    |-- fs.wasbs.impl: string (nullable = true)\n",
      " |    |-- ftp.blocksize: string (nullable = true)\n",
      " |    |-- ftp.bytes-per-checksum: string (nullable = true)\n",
      " |    |-- ftp.client-write-packet-size: string (nullable = true)\n",
      " |    |-- ftp.replication: string (nullable = true)\n",
      " |    |-- ftp.stream-buffer-size: string (nullable = true)\n",
      " |    |-- ha.failover-controller.active-standby-elector.zk.op.retries: string (nullable = true)\n",
      " |    |-- ha.failover-controller.cli-check.rpc-timeout.ms: string (nullable = true)\n",
      " |    |-- ha.failover-controller.graceful-fence.connection.retries: string (nullable = true)\n",
      " |    |-- ha.failover-controller.graceful-fence.rpc-timeout.ms: string (nullable = true)\n",
      " |    |-- ha.failover-controller.new-active.rpc-timeout.ms: string (nullable = true)\n",
      " |    |-- ha.health-monitor.check-interval.ms: string (nullable = true)\n",
      " |    |-- ha.health-monitor.connect-retry-interval.ms: string (nullable = true)\n",
      " |    |-- ha.health-monitor.rpc-timeout.ms: string (nullable = true)\n",
      " |    |-- ha.health-monitor.rpc.connect.max.retries: string (nullable = true)\n",
      " |    |-- ha.health-monitor.sleep-after-disconnect.ms: string (nullable = true)\n",
      " |    |-- ha.zookeeper.acl: string (nullable = true)\n",
      " |    |-- ha.zookeeper.parent-znode: string (nullable = true)\n",
      " |    |-- ha.zookeeper.session-timeout.ms: string (nullable = true)\n",
      " |    |-- hadoop.caller.context.enabled: string (nullable = true)\n",
      " |    |-- hadoop.caller.context.max.size: string (nullable = true)\n",
      " |    |-- hadoop.caller.context.signature.max.size: string (nullable = true)\n",
      " |    |-- hadoop.common.configuration.version: string (nullable = true)\n",
      " |    |-- hadoop.domainname.resolver.impl: string (nullable = true)\n",
      " |    |-- hadoop.http.authentication.kerberos.keytab: string (nullable = true)\n",
      " |    |-- hadoop.http.authentication.kerberos.principal: string (nullable = true)\n",
      " |    |-- hadoop.http.authentication.signature.secret.file: string (nullable = true)\n",
      " |    |-- hadoop.http.authentication.simple.anonymous.allowed: string (nullable = true)\n",
      " |    |-- hadoop.http.authentication.token.validity: string (nullable = true)\n",
      " |    |-- hadoop.http.authentication.type: string (nullable = true)\n",
      " |    |-- hadoop.http.cross-origin.allowed-headers: string (nullable = true)\n",
      " |    |-- hadoop.http.cross-origin.allowed-methods: string (nullable = true)\n",
      " |    |-- hadoop.http.cross-origin.allowed-origins: string (nullable = true)\n",
      " |    |-- hadoop.http.cross-origin.enabled: string (nullable = true)\n",
      " |    |-- hadoop.http.cross-origin.max-age: string (nullable = true)\n",
      " |    |-- hadoop.http.filter.initializers: string (nullable = true)\n",
      " |    |-- hadoop.http.idle_timeout.ms: string (nullable = true)\n",
      " |    |-- hadoop.http.logs.enabled: string (nullable = true)\n",
      " |    |-- hadoop.http.sni.host.check.enabled: string (nullable = true)\n",
      " |    |-- hadoop.http.staticuser.user: string (nullable = true)\n",
      " |    |-- hadoop.jetty.logs.serve.aliases: string (nullable = true)\n",
      " |    |-- hadoop.kerberos.keytab.login.autorenewal.enabled: string (nullable = true)\n",
      " |    |-- hadoop.kerberos.kinit.command: string (nullable = true)\n",
      " |    |-- hadoop.kerberos.min.seconds.before.relogin: string (nullable = true)\n",
      " |    |-- hadoop.metrics.jvm.use-thread-mxbean: string (nullable = true)\n",
      " |    |-- hadoop.prometheus.endpoint.enabled: string (nullable = true)\n",
      " |    |-- hadoop.registry.jaas.context: string (nullable = true)\n",
      " |    |-- hadoop.registry.secure: string (nullable = true)\n",
      " |    |-- hadoop.registry.system.acls: string (nullable = true)\n",
      " |    |-- hadoop.registry.zk.connection.timeout.ms: string (nullable = true)\n",
      " |    |-- hadoop.registry.zk.quorum: string (nullable = true)\n",
      " |    |-- hadoop.registry.zk.retry.ceiling.ms: string (nullable = true)\n",
      " |    |-- hadoop.registry.zk.retry.interval.ms: string (nullable = true)\n",
      " |    |-- hadoop.registry.zk.retry.times: string (nullable = true)\n",
      " |    |-- hadoop.registry.zk.root: string (nullable = true)\n",
      " |    |-- hadoop.registry.zk.session.timeout.ms: string (nullable = true)\n",
      " |    |-- hadoop.rpc.protection: string (nullable = true)\n",
      " |    |-- hadoop.rpc.socket.factory.class.default: string (nullable = true)\n",
      " |    |-- hadoop.security.auth_to_local.mechanism: string (nullable = true)\n",
      " |    |-- hadoop.security.authentication: string (nullable = true)\n",
      " |    |-- hadoop.security.authorization: string (nullable = true)\n",
      " |    |-- hadoop.security.credential.clear-text-fallback: string (nullable = true)\n",
      " |    |-- hadoop.security.crypto.buffer.size: string (nullable = true)\n",
      " |    |-- hadoop.security.crypto.cipher.suite: string (nullable = true)\n",
      " |    |-- hadoop.security.crypto.codec.classes.aes.ctr.nopadding: string (nullable = true)\n",
      " |    |-- hadoop.security.dns.log-slow-lookups.enabled: string (nullable = true)\n",
      " |    |-- hadoop.security.dns.log-slow-lookups.threshold.ms: string (nullable = true)\n",
      " |    |-- hadoop.security.group.mapping: string (nullable = true)\n",
      " |    |-- hadoop.security.group.mapping.ldap.connection.timeout.ms: string (nullable = true)\n",
      " |    |-- hadoop.security.group.mapping.ldap.conversion.rule: string (nullable = true)\n",
      " |    |-- hadoop.security.group.mapping.ldap.directory.search.timeout: string (nullable = true)\n",
      " |    |-- hadoop.security.group.mapping.ldap.num.attempts: string (nullable = true)\n",
      " |    |-- hadoop.security.group.mapping.ldap.num.attempts.before.failover: string (nullable = true)\n",
      " |    |-- hadoop.security.group.mapping.ldap.posix.attr.gid.name: string (nullable = true)\n",
      " |    |-- hadoop.security.group.mapping.ldap.posix.attr.uid.name: string (nullable = true)\n",
      " |    |-- hadoop.security.group.mapping.ldap.read.timeout.ms: string (nullable = true)\n",
      " |    |-- hadoop.security.group.mapping.ldap.search.attr.group.name: string (nullable = true)\n",
      " |    |-- hadoop.security.group.mapping.ldap.search.attr.member: string (nullable = true)\n",
      " |    |-- hadoop.security.group.mapping.ldap.search.filter.group: string (nullable = true)\n",
      " |    |-- hadoop.security.group.mapping.ldap.search.filter.user: string (nullable = true)\n",
      " |    |-- hadoop.security.group.mapping.ldap.search.group.hierarchy.levels: string (nullable = true)\n",
      " |    |-- hadoop.security.group.mapping.ldap.ssl: string (nullable = true)\n",
      " |    |-- hadoop.security.group.mapping.providers.combined: string (nullable = true)\n",
      " |    |-- hadoop.security.groups.cache.background.reload: string (nullable = true)\n",
      " |    |-- hadoop.security.groups.cache.background.reload.threads: string (nullable = true)\n",
      " |    |-- hadoop.security.groups.cache.secs: string (nullable = true)\n",
      " |    |-- hadoop.security.groups.cache.warn.after.ms: string (nullable = true)\n",
      " |    |-- hadoop.security.groups.negative-cache.secs: string (nullable = true)\n",
      " |    |-- hadoop.security.groups.shell.command.timeout: string (nullable = true)\n",
      " |    |-- hadoop.security.instrumentation.requires.admin: string (nullable = true)\n",
      " |    |-- hadoop.security.java.secure.random.algorithm: string (nullable = true)\n",
      " |    |-- hadoop.security.key.default.bitlength: string (nullable = true)\n",
      " |    |-- hadoop.security.key.default.cipher: string (nullable = true)\n",
      " |    |-- hadoop.security.kms.client.authentication.retry-count: string (nullable = true)\n",
      " |    |-- hadoop.security.kms.client.encrypted.key.cache.expiry: string (nullable = true)\n",
      " |    |-- hadoop.security.kms.client.encrypted.key.cache.low-watermark: string (nullable = true)\n",
      " |    |-- hadoop.security.kms.client.encrypted.key.cache.num.refill.threads: string (nullable = true)\n",
      " |    |-- hadoop.security.kms.client.encrypted.key.cache.size: string (nullable = true)\n",
      " |    |-- hadoop.security.kms.client.failover.sleep.base.millis: string (nullable = true)\n",
      " |    |-- hadoop.security.kms.client.failover.sleep.max.millis: string (nullable = true)\n",
      " |    |-- hadoop.security.kms.client.timeout: string (nullable = true)\n",
      " |    |-- hadoop.security.random.device.file.path: string (nullable = true)\n",
      " |    |-- hadoop.security.secure.random.impl: string (nullable = true)\n",
      " |    |-- hadoop.security.sensitive-config-keys: string (nullable = true)\n",
      " |    |-- hadoop.security.token.service.use_ip: string (nullable = true)\n",
      " |    |-- hadoop.security.uid.cache.secs: string (nullable = true)\n",
      " |    |-- hadoop.service.shutdown.timeout: string (nullable = true)\n",
      " |    |-- hadoop.shell.missing.defaultFs.warning: string (nullable = true)\n",
      " |    |-- hadoop.shell.safely.delete.limit.num.files: string (nullable = true)\n",
      " |    |-- hadoop.ssl.client.conf: string (nullable = true)\n",
      " |    |-- hadoop.ssl.enabled.protocols: string (nullable = true)\n",
      " |    |-- hadoop.ssl.hostname.verifier: string (nullable = true)\n",
      " |    |-- hadoop.ssl.keystores.factory.class: string (nullable = true)\n",
      " |    |-- hadoop.ssl.require.client.cert: string (nullable = true)\n",
      " |    |-- hadoop.ssl.server.conf: string (nullable = true)\n",
      " |    |-- hadoop.system.tags: string (nullable = true)\n",
      " |    |-- hadoop.tags.system: string (nullable = true)\n",
      " |    |-- hadoop.tmp.dir: string (nullable = true)\n",
      " |    |-- hadoop.user.group.static.mapping.overrides: string (nullable = true)\n",
      " |    |-- hadoop.util.hash.type: string (nullable = true)\n",
      " |    |-- hadoop.workaround.non.threadsafe.getpwuid: string (nullable = true)\n",
      " |    |-- hadoop.zk.acl: string (nullable = true)\n",
      " |    |-- hadoop.zk.num-retries: string (nullable = true)\n",
      " |    |-- hadoop.zk.retry-interval-ms: string (nullable = true)\n",
      " |    |-- hadoop.zk.timeout-ms: string (nullable = true)\n",
      " |    |-- hive.metastore.uris: string (nullable = true)\n",
      " |    |-- io.bytes.per.checksum: string (nullable = true)\n",
      " |    |-- io.compression.codec.bzip2.library: string (nullable = true)\n",
      " |    |-- io.erasurecode.codec.rs-legacy.rawcoders: string (nullable = true)\n",
      " |    |-- io.erasurecode.codec.rs.rawcoders: string (nullable = true)\n",
      " |    |-- io.erasurecode.codec.xor.rawcoders: string (nullable = true)\n",
      " |    |-- io.file.buffer.size: string (nullable = true)\n",
      " |    |-- io.map.index.interval: string (nullable = true)\n",
      " |    |-- io.map.index.skip: string (nullable = true)\n",
      " |    |-- io.mapfile.bloom.error.rate: string (nullable = true)\n",
      " |    |-- io.mapfile.bloom.size: string (nullable = true)\n",
      " |    |-- io.seqfile.compress.blocksize: string (nullable = true)\n",
      " |    |-- io.seqfile.local.dir: string (nullable = true)\n",
      " |    |-- io.serializations: string (nullable = true)\n",
      " |    |-- io.skip.checksum.errors: string (nullable = true)\n",
      " |    |-- ipc.[port_number].backoff.enable: string (nullable = true)\n",
      " |    |-- ipc.[port_number].callqueue.impl: string (nullable = true)\n",
      " |    |-- ipc.[port_number].cost-provider.impl: string (nullable = true)\n",
      " |    |-- ipc.[port_number].decay-scheduler.backoff.responsetime.enable: string (nullable = true)\n",
      " |    |-- ipc.[port_number].decay-scheduler.backoff.responsetime.thresholds: string (nullable = true)\n",
      " |    |-- ipc.[port_number].decay-scheduler.decay-factor: string (nullable = true)\n",
      " |    |-- ipc.[port_number].decay-scheduler.metrics.top.user.count: string (nullable = true)\n",
      " |    |-- ipc.[port_number].decay-scheduler.period-ms: string (nullable = true)\n",
      " |    |-- ipc.[port_number].decay-scheduler.thresholds: string (nullable = true)\n",
      " |    |-- ipc.[port_number].faircallqueue.multiplexer.weights: string (nullable = true)\n",
      " |    |-- ipc.[port_number].identity-provider.impl: string (nullable = true)\n",
      " |    |-- ipc.[port_number].scheduler.impl: string (nullable = true)\n",
      " |    |-- ipc.[port_number].scheduler.priority.levels: string (nullable = true)\n",
      " |    |-- ipc.[port_number].weighted-cost.handler: string (nullable = true)\n",
      " |    |-- ipc.[port_number].weighted-cost.lockexclusive: string (nullable = true)\n",
      " |    |-- ipc.[port_number].weighted-cost.lockfree: string (nullable = true)\n",
      " |    |-- ipc.[port_number].weighted-cost.lockshared: string (nullable = true)\n",
      " |    |-- ipc.[port_number].weighted-cost.response: string (nullable = true)\n",
      " |    |-- ipc.client.bind.wildcard.addr: string (nullable = true)\n",
      " |    |-- ipc.client.connect.max.retries: string (nullable = true)\n",
      " |    |-- ipc.client.connect.max.retries.on.timeouts: string (nullable = true)\n",
      " |    |-- ipc.client.connect.retry.interval: string (nullable = true)\n",
      " |    |-- ipc.client.connect.timeout: string (nullable = true)\n",
      " |    |-- ipc.client.connection.maxidletime: string (nullable = true)\n",
      " |    |-- ipc.client.fallback-to-simple-auth-allowed: string (nullable = true)\n",
      " |    |-- ipc.client.idlethreshold: string (nullable = true)\n",
      " |    |-- ipc.client.kill.max: string (nullable = true)\n",
      " |    |-- ipc.client.low-latency: string (nullable = true)\n",
      " |    |-- ipc.client.ping: string (nullable = true)\n",
      " |    |-- ipc.client.rpc-timeout.ms: string (nullable = true)\n",
      " |    |-- ipc.client.tcpnodelay: string (nullable = true)\n",
      " |    |-- ipc.maximum.data.length: string (nullable = true)\n",
      " |    |-- ipc.maximum.response.length: string (nullable = true)\n",
      " |    |-- ipc.ping.interval: string (nullable = true)\n",
      " |    |-- ipc.server.listen.queue.size: string (nullable = true)\n",
      " |    |-- ipc.server.log.slow.rpc: string (nullable = true)\n",
      " |    |-- ipc.server.max.connections: string (nullable = true)\n",
      " |    |-- ipc.server.purge.interval: string (nullable = true)\n",
      " |    |-- ipc.server.reuseaddr: string (nullable = true)\n",
      " |    |-- javax.jdo.option.ConnectionDriverName: string (nullable = true)\n",
      " |    |-- javax.jdo.option.ConnectionPassword: string (nullable = true)\n",
      " |    |-- javax.jdo.option.ConnectionURL: string (nullable = true)\n",
      " |    |-- javax.jdo.option.ConnectionUserName: string (nullable = true)\n",
      " |    |-- map.sort.class: string (nullable = true)\n",
      " |    |-- mapreduce.am.max-attempts: string (nullable = true)\n",
      " |    |-- mapreduce.app-submission.cross-platform: string (nullable = true)\n",
      " |    |-- mapreduce.client.completion.pollinterval: string (nullable = true)\n",
      " |    |-- mapreduce.client.libjars.wildcard: string (nullable = true)\n",
      " |    |-- mapreduce.client.output.filter: string (nullable = true)\n",
      " |    |-- mapreduce.client.progressmonitor.pollinterval: string (nullable = true)\n",
      " |    |-- mapreduce.client.submit.file.replication: string (nullable = true)\n",
      " |    |-- mapreduce.cluster.acls.enabled: string (nullable = true)\n",
      " |    |-- mapreduce.cluster.local.dir: string (nullable = true)\n",
      " |    |-- mapreduce.fileoutputcommitter.algorithm.version: string (nullable = true)\n",
      " |    |-- mapreduce.fileoutputcommitter.task.cleanup.enabled: string (nullable = true)\n",
      " |    |-- mapreduce.framework.name: string (nullable = true)\n",
      " |    |-- mapreduce.ifile.readahead: string (nullable = true)\n",
      " |    |-- mapreduce.ifile.readahead.bytes: string (nullable = true)\n",
      " |    |-- mapreduce.input.fileinputformat.list-status.num-threads: string (nullable = true)\n",
      " |    |-- mapreduce.input.fileinputformat.split.minsize: string (nullable = true)\n",
      " |    |-- mapreduce.input.lineinputformat.linespermap: string (nullable = true)\n",
      " |    |-- mapreduce.job.acl-modify-job: string (nullable = true)\n",
      " |    |-- mapreduce.job.acl-view-job: string (nullable = true)\n",
      " |    |-- mapreduce.job.cache.limit.max-resources: string (nullable = true)\n",
      " |    |-- mapreduce.job.cache.limit.max-resources-mb: string (nullable = true)\n",
      " |    |-- mapreduce.job.cache.limit.max-single-resource-mb: string (nullable = true)\n",
      " |    |-- mapreduce.job.classloader: string (nullable = true)\n",
      " |    |-- mapreduce.job.committer.setup.cleanup.needed: string (nullable = true)\n",
      " |    |-- mapreduce.job.complete.cancel.delegation.tokens: string (nullable = true)\n",
      " |    |-- mapreduce.job.counters.max: string (nullable = true)\n",
      " |    |-- mapreduce.job.dfs.storage.capacity.kill-limit-exceed: string (nullable = true)\n",
      " |    |-- mapreduce.job.emit-timeline-data: string (nullable = true)\n",
      " |    |-- mapreduce.job.encrypted-intermediate-data: string (nullable = true)\n",
      " |    |-- mapreduce.job.encrypted-intermediate-data-key-size-bits: string (nullable = true)\n",
      " |    |-- mapreduce.job.encrypted-intermediate-data.buffer.kb: string (nullable = true)\n",
      " |    |-- mapreduce.job.end-notification.max.attempts: string (nullable = true)\n",
      " |    |-- mapreduce.job.end-notification.max.retry.interval: string (nullable = true)\n",
      " |    |-- mapreduce.job.end-notification.retry.attempts: string (nullable = true)\n",
      " |    |-- mapreduce.job.end-notification.retry.interval: string (nullable = true)\n",
      " |    |-- mapreduce.job.finish-when-all-reducers-done: string (nullable = true)\n",
      " |    |-- mapreduce.job.hdfs-servers: string (nullable = true)\n",
      " |    |-- mapreduce.job.heap.memory-mb.ratio: string (nullable = true)\n",
      " |    |-- mapreduce.job.local-fs.single-disk-limit.bytes: string (nullable = true)\n",
      " |    |-- mapreduce.job.local-fs.single-disk-limit.check.interval-ms: string (nullable = true)\n",
      " |    |-- mapreduce.job.local-fs.single-disk-limit.check.kill-limit-exceed: string (nullable = true)\n",
      " |    |-- mapreduce.job.map.output.collector.class: string (nullable = true)\n",
      " |    |-- mapreduce.job.maps: string (nullable = true)\n",
      " |    |-- mapreduce.job.max.map: string (nullable = true)\n",
      " |    |-- mapreduce.job.max.split.locations: string (nullable = true)\n",
      " |    |-- mapreduce.job.maxtaskfailures.per.tracker: string (nullable = true)\n",
      " |    |-- mapreduce.job.queuename: string (nullable = true)\n",
      " |    |-- mapreduce.job.reduce.shuffle.consumer.plugin.class: string (nullable = true)\n",
      " |    |-- mapreduce.job.reduce.slowstart.completedmaps: string (nullable = true)\n",
      " |    |-- mapreduce.job.reducer.preempt.delay.sec: string (nullable = true)\n",
      " |    |-- mapreduce.job.reducer.unconditional-preempt.delay.sec: string (nullable = true)\n",
      " |    |-- mapreduce.job.reduces: string (nullable = true)\n",
      " |    |-- mapreduce.job.running.map.limit: string (nullable = true)\n",
      " |    |-- mapreduce.job.running.reduce.limit: string (nullable = true)\n",
      " |    |-- mapreduce.job.sharedcache.mode: string (nullable = true)\n",
      " |    |-- mapreduce.job.speculative.minimum-allowed-tasks: string (nullable = true)\n",
      " |    |-- mapreduce.job.speculative.retry-after-no-speculate: string (nullable = true)\n",
      " |    |-- mapreduce.job.speculative.retry-after-speculate: string (nullable = true)\n",
      " |    |-- mapreduce.job.speculative.slowtaskthreshold: string (nullable = true)\n",
      " |    |-- mapreduce.job.speculative.speculative-cap-running-tasks: string (nullable = true)\n",
      " |    |-- mapreduce.job.speculative.speculative-cap-total-tasks: string (nullable = true)\n",
      " |    |-- mapreduce.job.split.metainfo.maxsize: string (nullable = true)\n",
      " |    |-- mapreduce.job.token.tracking.ids.enabled: string (nullable = true)\n",
      " |    |-- mapreduce.job.ubertask.enable: string (nullable = true)\n",
      " |    |-- mapreduce.job.ubertask.maxmaps: string (nullable = true)\n",
      " |    |-- mapreduce.job.ubertask.maxreduces: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.address: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.admin.acl: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.admin.address: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.always-scan-user-dir: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.cleaner.enable: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.cleaner.interval-ms: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.client.thread-count: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.datestring.cache.size: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.done-dir: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.http.policy: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.intermediate-done-dir: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.intermediate-user-done-dir.permissions: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.jhist.format: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.joblist.cache.size: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.jobname.limit: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.keytab: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.loadedjob.tasks.max: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.loadedjobs.cache.size: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.max-age-ms: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.minicluster.fixed.ports: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.move.interval-ms: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.move.thread-count: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.principal: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.recovery.enable: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.recovery.store.class: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.recovery.store.fs.uri: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.recovery.store.leveldb.path: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.webapp.address: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.webapp.https.address: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.webapp.rest-csrf.custom-header: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.webapp.rest-csrf.enabled: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.webapp.rest-csrf.methods-to-ignore: string (nullable = true)\n",
      " |    |-- mapreduce.jobhistory.webapp.xfs-filter.xframe-options: string (nullable = true)\n",
      " |    |-- mapreduce.jvm.system-properties-to-log: string (nullable = true)\n",
      " |    |-- mapreduce.map.cpu.vcores: string (nullable = true)\n",
      " |    |-- mapreduce.map.log.level: string (nullable = true)\n",
      " |    |-- mapreduce.map.maxattempts: string (nullable = true)\n",
      " |    |-- mapreduce.map.memory.mb: string (nullable = true)\n",
      " |    |-- mapreduce.map.output.compress: string (nullable = true)\n",
      " |    |-- mapreduce.map.output.compress.codec: string (nullable = true)\n",
      " |    |-- mapreduce.map.skip.maxrecords: string (nullable = true)\n",
      " |    |-- mapreduce.map.skip.proc-count.auto-incr: string (nullable = true)\n",
      " |    |-- mapreduce.map.sort.spill.percent: string (nullable = true)\n",
      " |    |-- mapreduce.map.speculative: string (nullable = true)\n",
      " |    |-- mapreduce.output.fileoutputformat.compress: string (nullable = true)\n",
      " |    |-- mapreduce.output.fileoutputformat.compress.codec: string (nullable = true)\n",
      " |    |-- mapreduce.output.fileoutputformat.compress.type: string (nullable = true)\n",
      " |    |-- mapreduce.outputcommitter.factory.scheme.s3a: string (nullable = true)\n",
      " |    |-- mapreduce.reduce.cpu.vcores: string (nullable = true)\n",
      " |    |-- mapreduce.reduce.input.buffer.percent: string (nullable = true)\n",
      " |    |-- mapreduce.reduce.log.level: string (nullable = true)\n",
      " |    |-- mapreduce.reduce.markreset.buffer.percent: string (nullable = true)\n",
      " |    |-- mapreduce.reduce.maxattempts: string (nullable = true)\n",
      " |    |-- mapreduce.reduce.memory.mb: string (nullable = true)\n",
      " |    |-- mapreduce.reduce.merge.inmem.threshold: string (nullable = true)\n",
      " |    |-- mapreduce.reduce.shuffle.connect.timeout: string (nullable = true)\n",
      " |    |-- mapreduce.reduce.shuffle.fetch.retry.enabled: string (nullable = true)\n",
      " |    |-- mapreduce.reduce.shuffle.fetch.retry.interval-ms: string (nullable = true)\n",
      " |    |-- mapreduce.reduce.shuffle.fetch.retry.timeout-ms: string (nullable = true)\n",
      " |    |-- mapreduce.reduce.shuffle.input.buffer.percent: string (nullable = true)\n",
      " |    |-- mapreduce.reduce.shuffle.memory.limit.percent: string (nullable = true)\n",
      " |    |-- mapreduce.reduce.shuffle.merge.percent: string (nullable = true)\n",
      " |    |-- mapreduce.reduce.shuffle.parallelcopies: string (nullable = true)\n",
      " |    |-- mapreduce.reduce.shuffle.read.timeout: string (nullable = true)\n",
      " |    |-- mapreduce.reduce.shuffle.retry-delay.max.ms: string (nullable = true)\n",
      " |    |-- mapreduce.reduce.skip.maxgroups: string (nullable = true)\n",
      " |    |-- mapreduce.reduce.skip.proc-count.auto-incr: string (nullable = true)\n",
      " |    |-- mapreduce.reduce.speculative: string (nullable = true)\n",
      " |    |-- mapreduce.shuffle.connection-keep-alive.enable: string (nullable = true)\n",
      " |    |-- mapreduce.shuffle.connection-keep-alive.timeout: string (nullable = true)\n",
      " |    |-- mapreduce.shuffle.listen.queue.size: string (nullable = true)\n",
      " |    |-- mapreduce.shuffle.max.connections: string (nullable = true)\n",
      " |    |-- mapreduce.shuffle.max.threads: string (nullable = true)\n",
      " |    |-- mapreduce.shuffle.pathcache.concurrency-level: string (nullable = true)\n",
      " |    |-- mapreduce.shuffle.pathcache.expire-after-access-minutes: string (nullable = true)\n",
      " |    |-- mapreduce.shuffle.pathcache.max-weight: string (nullable = true)\n",
      " |    |-- mapreduce.shuffle.port: string (nullable = true)\n",
      " |    |-- mapreduce.shuffle.ssl.enabled: string (nullable = true)\n",
      " |    |-- mapreduce.shuffle.ssl.file.buffer.size: string (nullable = true)\n",
      " |    |-- mapreduce.shuffle.transfer.buffer.size: string (nullable = true)\n",
      " |    |-- mapreduce.task.combine.progress.records: string (nullable = true)\n",
      " |    |-- mapreduce.task.exit.timeout: string (nullable = true)\n",
      " |    |-- mapreduce.task.exit.timeout.check-interval-ms: string (nullable = true)\n",
      " |    |-- mapreduce.task.files.preserve.failedtasks: string (nullable = true)\n",
      " |    |-- mapreduce.task.io.sort.factor: string (nullable = true)\n",
      " |    |-- mapreduce.task.io.sort.mb: string (nullable = true)\n",
      " |    |-- mapreduce.task.local-fs.write-limit.bytes: string (nullable = true)\n",
      " |    |-- mapreduce.task.merge.progress.records: string (nullable = true)\n",
      " |    |-- mapreduce.task.profile: string (nullable = true)\n",
      " |    |-- mapreduce.task.profile.map.params: string (nullable = true)\n",
      " |    |-- mapreduce.task.profile.maps: string (nullable = true)\n",
      " |    |-- mapreduce.task.profile.params: string (nullable = true)\n",
      " |    |-- mapreduce.task.profile.reduce.params: string (nullable = true)\n",
      " |    |-- mapreduce.task.profile.reduces: string (nullable = true)\n",
      " |    |-- mapreduce.task.skip.start.attempts: string (nullable = true)\n",
      " |    |-- mapreduce.task.stuck.timeout-ms: string (nullable = true)\n",
      " |    |-- mapreduce.task.timeout: string (nullable = true)\n",
      " |    |-- mapreduce.task.userlog.limit.kb: string (nullable = true)\n",
      " |    |-- net.topology.impl: string (nullable = true)\n",
      " |    |-- net.topology.node.switch.mapping.impl: string (nullable = true)\n",
      " |    |-- net.topology.script.number.args: string (nullable = true)\n",
      " |    |-- nfs.exports.allowed.hosts: string (nullable = true)\n",
      " |    |-- rpc.engine.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB: string (nullable = true)\n",
      " |    |-- rpc.metrics.quantile.enable: string (nullable = true)\n",
      " |    |-- rpc.metrics.timeunit: string (nullable = true)\n",
      " |    |-- seq.io.sort.factor: string (nullable = true)\n",
      " |    |-- seq.io.sort.mb: string (nullable = true)\n",
      " |    |-- tfile.fs.input.buffer.size: string (nullable = true)\n",
      " |    |-- tfile.fs.output.buffer.size: string (nullable = true)\n",
      " |    |-- tfile.io.chunk.size: string (nullable = true)\n",
      " |    |-- yarn.acl.enable: string (nullable = true)\n",
      " |    |-- yarn.acl.reservation-enable: string (nullable = true)\n",
      " |    |-- yarn.admin.acl: string (nullable = true)\n",
      " |    |-- yarn.am.liveness-monitor.expiry-interval-ms: string (nullable = true)\n",
      " |    |-- yarn.app.attempt.diagnostics.limit.kc: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.am.command-opts: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.am.container.log.backups: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.am.container.log.limit.kb: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.am.hard-kill-timeout-ms: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.am.job.committer.cancel-timeout: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.am.job.committer.commit-window: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.am.job.task.listener.thread-count: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.am.log.level: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.am.resource.cpu-vcores: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.am.resource.mb: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.am.staging-dir: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.am.staging-dir.erasurecoding.enabled: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.am.webapp.https.client.auth: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.am.webapp.https.enabled: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.client-am.ipc.max-retries: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.client.job.max-retries: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.client.job.retry-interval: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.client.max-retries: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.shuffle.log.backups: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.shuffle.log.limit.kb: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.shuffle.log.separate: string (nullable = true)\n",
      " |    |-- yarn.app.mapreduce.task.container.log.backups: string (nullable = true)\n",
      " |    |-- yarn.client.application-client-protocol.poll-interval-ms: string (nullable = true)\n",
      " |    |-- yarn.client.application-client-protocol.poll-timeout-ms: string (nullable = true)\n",
      " |    |-- yarn.client.failover-no-ha-proxy-provider: string (nullable = true)\n",
      " |    |-- yarn.client.failover-proxy-provider: string (nullable = true)\n",
      " |    |-- yarn.client.failover-retries: string (nullable = true)\n",
      " |    |-- yarn.client.failover-retries-on-socket-timeouts: string (nullable = true)\n",
      " |    |-- yarn.client.load.resource-types.from-server: string (nullable = true)\n",
      " |    |-- yarn.client.max-cached-nodemanagers-proxies: string (nullable = true)\n",
      " |    |-- yarn.client.nodemanager-client-async.thread-pool-max-size: string (nullable = true)\n",
      " |    |-- yarn.client.nodemanager-connect.max-wait-ms: string (nullable = true)\n",
      " |    |-- yarn.client.nodemanager-connect.retry-interval-ms: string (nullable = true)\n",
      " |    |-- yarn.cluster.max-application-priority: string (nullable = true)\n",
      " |    |-- yarn.dispatcher.cpu-monitor.samples-per-min: string (nullable = true)\n",
      " |    |-- yarn.dispatcher.drain-events.timeout: string (nullable = true)\n",
      " |    |-- yarn.dispatcher.print-events-info.threshold: string (nullable = true)\n",
      " |    |-- yarn.fail-fast: string (nullable = true)\n",
      " |    |-- yarn.federation.cache-ttl.secs: string (nullable = true)\n",
      " |    |-- yarn.federation.enabled: string (nullable = true)\n",
      " |    |-- yarn.federation.registry.base-dir: string (nullable = true)\n",
      " |    |-- yarn.federation.state-store.class: string (nullable = true)\n",
      " |    |-- yarn.federation.subcluster-resolver.class: string (nullable = true)\n",
      " |    |-- yarn.http.policy: string (nullable = true)\n",
      " |    |-- yarn.intermediate-data-encryption.enable: string (nullable = true)\n",
      " |    |-- yarn.ipc.rpc.class: string (nullable = true)\n",
      " |    |-- yarn.is.minicluster: string (nullable = true)\n",
      " |    |-- yarn.log-aggregation-enable: string (nullable = true)\n",
      " |    |-- yarn.log-aggregation-status.time-out.ms: string (nullable = true)\n",
      " |    |-- yarn.log-aggregation.debug.filesize: string (nullable = true)\n",
      " |    |-- yarn.log-aggregation.file-controller.TFile.class: string (nullable = true)\n",
      " |    |-- yarn.log-aggregation.file-formats: string (nullable = true)\n",
      " |    |-- yarn.log-aggregation.retain-check-interval-seconds: string (nullable = true)\n",
      " |    |-- yarn.log-aggregation.retain-seconds: string (nullable = true)\n",
      " |    |-- yarn.minicluster.control-resource-monitoring: string (nullable = true)\n",
      " |    |-- yarn.minicluster.fixed.ports: string (nullable = true)\n",
      " |    |-- yarn.minicluster.use-rpc: string (nullable = true)\n",
      " |    |-- yarn.minicluster.yarn.nodemanager.resource.memory-mb: string (nullable = true)\n",
      " |    |-- yarn.nm.liveness-monitor.expiry-interval-ms: string (nullable = true)\n",
      " |    |-- yarn.node-attribute.fs-store.impl.class: string (nullable = true)\n",
      " |    |-- yarn.node-labels.configuration-type: string (nullable = true)\n",
      " |    |-- yarn.node-labels.enabled: string (nullable = true)\n",
      " |    |-- yarn.node-labels.fs-store.impl.class: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.address: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.admin-env: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.amrmproxy.address: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.amrmproxy.client.thread-count: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.amrmproxy.enabled: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.amrmproxy.ha.enable: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.amrmproxy.interceptor-class.pipeline: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.aux-services.manifest.enabled: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.aux-services.manifest.reload-ms: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.aux-services.mapreduce_shuffle.class: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.collector-service.address: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.collector-service.thread-count: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.container-diagnostics-maximum-size: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.container-executor.class: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.container-executor.exit-code-file.timeout-ms: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.container-localizer.java.opts: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.container-localizer.log.level: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.container-log-monitor.dir-size-limit-bytes: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.container-log-monitor.enable: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.container-log-monitor.interval-ms: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.container-log-monitor.total-size-limit-bytes: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.container-manager.thread-count: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.container-metrics.enable: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.container-metrics.period-ms: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.container-metrics.unregister-delay-ms: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.container-monitor.enabled: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.container-retry-minimum-interval-ms: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.container.stderr.pattern: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.container.stderr.tail.bytes: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.containers-launcher.class: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.default-container-executor.log-dirs.permissions: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.delete.debug-delay-sec: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.delete.thread-count: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.disk-health-checker.disk-free-space-threshold.enabled: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.disk-health-checker.disk-utilization-threshold.enabled: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.disk-health-checker.enable: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.disk-health-checker.interval-ms: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.disk-health-checker.min-free-space-per-disk-watermark-high-mb: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.disk-health-checker.min-healthy-disks: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.disk-validator: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.distributed-scheduling.enabled: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.elastic-memory-control.enabled: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.elastic-memory-control.oom-handler: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.elastic-memory-control.timeout-sec: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.emit-container-events: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.env-whitelist: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.health-checker.interval-ms: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.health-checker.run-before-startup: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.health-checker.scripts: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.health-checker.timeout-ms: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.hostname: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.keytab: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.linux-container-executor.cgroups.delete-delay-ms: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.linux-container-executor.cgroups.delete-timeout-ms: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.linux-container-executor.cgroups.hierarchy: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.linux-container-executor.cgroups.mount: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.linux-container-executor.resources-handler.class: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.local-cache.max-files-per-directory: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.local-dirs: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.localizer.address: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.localizer.cache.cleanup.interval-ms: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.localizer.cache.target-size-mb: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.localizer.client.thread-count: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.localizer.fetch.thread-count: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.log-aggregation.compression-type: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.log-aggregation.num-log-files-per-app: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.log-aggregation.policy.class: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds.min: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.log-container-debug-info.enabled: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.log-dirs: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.log.deletion-threads-count: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.log.retain-seconds: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.logaggregation.threadpool-size-max: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.node-attributes.provider.fetch-interval-ms: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.node-attributes.provider.fetch-timeout-ms: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.node-attributes.resync-interval-ms: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.node-labels.provider.fetch-interval-ms: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.node-labels.provider.fetch-timeout-ms: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.node-labels.resync-interval-ms: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.numa-awareness.enabled: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.numa-awareness.numactl.cmd: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.numa-awareness.read-topology: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.opportunistic-containers-max-queue-length: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.opportunistic-containers-use-pause-for-preemption: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.pluggable-device-framework.enabled: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.pmem-check-enabled: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.process-kill-wait.ms: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.recovery.compaction-interval-secs: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.recovery.dir: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.recovery.enabled: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.recovery.supervised: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.remote-app-log-dir: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.remote-app-log-dir-include-older: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.remote-app-log-dir-suffix: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.resource-monitor.interval-ms: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.resource-plugins.fpga.allowed-fpga-devices: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.resource-plugins.fpga.vendor-plugin.class: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.resource-plugins.gpu.allowed-gpu-devices: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.resource-plugins.gpu.docker-plugin: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.resource-plugins.gpu.docker-plugin.nvidia-docker-v1.endpoint: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.resource.count-logical-processors-as-cores: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.resource.cpu-vcores: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.resource.detect-hardware-capabilities: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.resource.memory-mb: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.resource.memory.cgroups.soft-limit-percentage: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.resource.memory.cgroups.swappiness: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.resource.memory.enabled: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.resource.memory.enforced: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.resource.pcores-vcores-multiplier: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.resource.percentage-physical-cpu-limit: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.resource.system-reserved-memory-mb: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.resourcemanager.minimum.version: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.allowed-runtimes: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.docker.allowed-container-networks: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.docker.allowed-container-runtimes: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.docker.capabilities: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.docker.default-container-network: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.docker.delayed-removal.allowed: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.docker.enable-userremapping.allowed: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.docker.host-pid-namespace.allowed: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.docker.image-update: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.docker.privileged-containers.allowed: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.docker.stop.grace-period: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.docker.userremapping-gid-threshold: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.docker.userremapping-uid-threshold: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.runc.allowed-container-networks: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.runc.allowed-container-runtimes: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.runc.hdfs-manifest-to-resources-plugin.stat-cache-size: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.runc.hdfs-manifest-to-resources-plugin.stat-cache-timeout-interval-secs: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.runc.host-pid-namespace.allowed: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.cache-refresh-interval-secs: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.hdfs-hash-file: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.num-manifests-to-cache: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.runc.image-toplevel-dir: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.runc.layer-mounts-interval-secs: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.runc.layer-mounts-to-keep: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.runc.manifest-to-resources-plugin: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.runc.privileged-containers.allowed: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.sandbox-mode: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.runtime.linux.sandbox-mode.local-dirs.permissions: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.sleep-delay-before-sigkill.ms: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.vmem-check-enabled: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.vmem-pmem-ratio: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.webapp.address: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.webapp.cross-origin.enabled: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.webapp.https.address: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.webapp.rest-csrf.custom-header: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.webapp.rest-csrf.enabled: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.webapp.rest-csrf.methods-to-ignore: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.webapp.xfs-filter.xframe-options: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.windows-container.cpu-limit.enabled: string (nullable = true)\n",
      " |    |-- yarn.nodemanager.windows-container.memory-limit.enabled: string (nullable = true)\n",
      " |    |-- yarn.registry.class: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.activities-manager.app-activities.max-queue-length: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.activities-manager.app-activities.ttl-ms: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.activities-manager.cleanup-interval-ms: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.activities-manager.scheduler-activities.ttl-ms: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.address: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.admin.address: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.admin.client.thread-count: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.am.max-attempts: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.amlauncher.thread-count: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.application-https.policy: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.application-tag-based-placement.enable: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.application-timeouts.monitor.interval-ms: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.application.max-tag.length: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.application.max-tags: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.auto-update.containers: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.client.thread-count: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.configuration.file-system-based-store: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.configuration.provider-class: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.connect.max-wait.ms: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.connect.retry-interval.ms: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.container.liveness-monitor.interval-ms: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.decommissioning-nodes-watcher.poll-interval-secs: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.delayed.delegation-token.removal-interval-ms: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.delegation-token-renewer.thread-count: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.delegation-token-renewer.thread-retry-interval: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.delegation-token-renewer.thread-retry-max-attempts: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.delegation-token-renewer.thread-timeout: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.delegation-token.always-cancel: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.delegation-token.max-conf-size-bytes: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.delegation.key.update-interval: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.delegation.token.max-lifetime: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.delegation.token.renew-interval: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.epoch.range: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.fail-fast: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.fs.state-store.num-retries: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.fs.state-store.retry-interval-ms: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.fs.state-store.uri: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.ha.automatic-failover.embedded: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.ha.automatic-failover.enabled: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.ha.automatic-failover.zk-base-path: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.ha.enabled: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.history-writer.multi-threaded-dispatcher.pool-size: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.hostname: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.keytab: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.leveldb-state-store.compaction-interval-secs: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.leveldb-state-store.path: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.max-completed-applications: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.max-log-aggregation-diagnostics-in-memory: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.metrics.runtime.buckets: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.nm-container-queuing.load-comparator: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.nm-container-queuing.max-queue-length: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.nm-container-queuing.max-queue-wait-time-ms: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.nm-container-queuing.min-queue-length: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.nm-container-queuing.min-queue-wait-time-ms: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.nm-container-queuing.queue-limit-stdev: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.nm-container-queuing.sorting-nodes-interval-ms: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.nm-tokens.master-key-rolling-interval-secs: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.node-ip-cache.expiry-interval-secs: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.node-labels.provider.fetch-interval-ms: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.node-removal-untracked.timeout-ms: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.nodemanager-connect-retries: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.nodemanager.minimum.version: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.nodemanagers.heartbeat-interval-max-ms: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.nodemanagers.heartbeat-interval-min-ms: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.nodemanagers.heartbeat-interval-ms: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.nodemanagers.heartbeat-interval-scaling-enable: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.nodemanagers.heartbeat-interval-slowdown-factor: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.nodemanagers.heartbeat-interval-speedup-factor: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.opportunistic-container-allocation.enabled: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.opportunistic-container-allocation.nodes-used: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.opportunistic.max.container-allocation.per.am.heartbeat: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.placement-constraints.algorithm.class: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.placement-constraints.algorithm.iterator: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.placement-constraints.algorithm.pool-size: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.placement-constraints.handler: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.placement-constraints.retry-attempts: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.placement-constraints.scheduler.pool-size: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.proxy-user-privileges.enabled: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.proxy.connection.timeout: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.proxy.timeout.enabled: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.recovery.enabled: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.reservation-system.enable: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.reservation-system.planfollower.time-step: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.resource-profiles.enabled: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.resource-profiles.source-file: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.resource-tracker.address: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.resource-tracker.client.thread-count: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.resource-tracker.nm.ip-hostname-check: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.rm.container-allocation.expiry-interval-ms: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.scheduler.address: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.scheduler.class: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.scheduler.client.thread-count: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.scheduler.monitor.enable: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.scheduler.monitor.policies: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.state-store.max-completed-applications: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.store.class: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.submission-preprocessor.enabled: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.submission-preprocessor.file-refresh-interval-ms: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.system-metrics-publisher.enabled: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.system-metrics-publisher.timeline-server-v1.batch-size: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.system-metrics-publisher.timeline-server-v1.enable-batch: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.system-metrics-publisher.timeline-server-v1.interval-seconds: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.webapp.address: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.webapp.cross-origin.enabled: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.webapp.https.address: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.webapp.rest-csrf.custom-header: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.webapp.rest-csrf.enabled: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.webapp.rest-csrf.methods-to-ignore: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.webapp.ui-actions.enabled: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.webapp.xfs-filter.xframe-options: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.work-preserving-recovery.enabled: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.zk-appid-node.split-index: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.zk-delegation-token-node.split-index: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.zk-max-znode-size.bytes: string (nullable = true)\n",
      " |    |-- yarn.resourcemanager.zk-state-store.parent-path: string (nullable = true)\n",
      " |    |-- yarn.rm.system-metrics-publisher.emit-container-events: string (nullable = true)\n",
      " |    |-- yarn.router.clientrm.interceptor-class.pipeline: string (nullable = true)\n",
      " |    |-- yarn.router.interceptor.user.threadpool-size: string (nullable = true)\n",
      " |    |-- yarn.router.pipeline.cache-max-size: string (nullable = true)\n",
      " |    |-- yarn.router.rmadmin.interceptor-class.pipeline: string (nullable = true)\n",
      " |    |-- yarn.router.webapp.address: string (nullable = true)\n",
      " |    |-- yarn.router.webapp.https.address: string (nullable = true)\n",
      " |    |-- yarn.router.webapp.interceptor-class.pipeline: string (nullable = true)\n",
      " |    |-- yarn.scheduler.configuration.fs.path: string (nullable = true)\n",
      " |    |-- yarn.scheduler.configuration.leveldb-store.compaction-interval-secs: string (nullable = true)\n",
      " |    |-- yarn.scheduler.configuration.leveldb-store.path: string (nullable = true)\n",
      " |    |-- yarn.scheduler.configuration.max.version: string (nullable = true)\n",
      " |    |-- yarn.scheduler.configuration.mutation.acl-policy.class: string (nullable = true)\n",
      " |    |-- yarn.scheduler.configuration.store.class: string (nullable = true)\n",
      " |    |-- yarn.scheduler.configuration.store.max-logs: string (nullable = true)\n",
      " |    |-- yarn.scheduler.configuration.zk-store.parent-path: string (nullable = true)\n",
      " |    |-- yarn.scheduler.include-port-in-node-name: string (nullable = true)\n",
      " |    |-- yarn.scheduler.maximum-allocation-mb: string (nullable = true)\n",
      " |    |-- yarn.scheduler.maximum-allocation-vcores: string (nullable = true)\n",
      " |    |-- yarn.scheduler.minimum-allocation-mb: string (nullable = true)\n",
      " |    |-- yarn.scheduler.minimum-allocation-vcores: string (nullable = true)\n",
      " |    |-- yarn.scheduler.queue-placement-rules: string (nullable = true)\n",
      " |    |-- yarn.sharedcache.admin.address: string (nullable = true)\n",
      " |    |-- yarn.sharedcache.admin.thread-count: string (nullable = true)\n",
      " |    |-- yarn.sharedcache.app-checker.class: string (nullable = true)\n",
      " |    |-- yarn.sharedcache.checksum.algo.impl: string (nullable = true)\n",
      " |    |-- yarn.sharedcache.cleaner.initial-delay-mins: string (nullable = true)\n",
      " |    |-- yarn.sharedcache.cleaner.period-mins: string (nullable = true)\n",
      " |    |-- yarn.sharedcache.cleaner.resource-sleep-ms: string (nullable = true)\n",
      " |    |-- yarn.sharedcache.client-server.address: string (nullable = true)\n",
      " |    |-- yarn.sharedcache.client-server.thread-count: string (nullable = true)\n",
      " |    |-- yarn.sharedcache.enabled: string (nullable = true)\n",
      " |    |-- yarn.sharedcache.nested-level: string (nullable = true)\n",
      " |    |-- yarn.sharedcache.nm.uploader.replication.factor: string (nullable = true)\n",
      " |    |-- yarn.sharedcache.nm.uploader.thread-count: string (nullable = true)\n",
      " |    |-- yarn.sharedcache.root-dir: string (nullable = true)\n",
      " |    |-- yarn.sharedcache.store.class: string (nullable = true)\n",
      " |    |-- yarn.sharedcache.store.in-memory.check-period-mins: string (nullable = true)\n",
      " |    |-- yarn.sharedcache.store.in-memory.initial-delay-mins: string (nullable = true)\n",
      " |    |-- yarn.sharedcache.store.in-memory.staleness-period-mins: string (nullable = true)\n",
      " |    |-- yarn.sharedcache.uploader.server.address: string (nullable = true)\n",
      " |    |-- yarn.sharedcache.uploader.server.thread-count: string (nullable = true)\n",
      " |    |-- yarn.sharedcache.webapp.address: string (nullable = true)\n",
      " |    |-- yarn.system-metrics-publisher.enabled: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.address: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.app-aggregation-interval-secs: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.app-collector.linger-period.ms: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.client.best-effort: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.client.drain-entities.timeout.ms: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.client.fd-clean-interval-secs: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.client.fd-flush-interval-secs: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.client.fd-retain-secs: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.client.internal-timers-ttl-secs: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.client.max-retries: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.client.retry-interval-ms: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.enabled: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.entity-group-fs-store.active-dir: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.entity-group-fs-store.app-cache-size: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.entity-group-fs-store.cache-store-class: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.entity-group-fs-store.cleaner-interval-seconds: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.entity-group-fs-store.done-dir: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.entity-group-fs-store.leveldb-cache-read-cache-size: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.entity-group-fs-store.retain-seconds: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.entity-group-fs-store.scan-interval-seconds: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.entity-group-fs-store.summary-store: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.entity-group-fs-store.with-user-dir: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.flowname.max-size: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.generic-application-history.max-applications: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.handler-thread-count: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.hbase-schema.prefix: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.hbase.coprocessor.app-final-value-retention-milliseconds: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.hbase.coprocessor.jar.hdfs.location: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.hostname: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.http-authentication.simple.anonymous.allowed: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.http-authentication.type: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.http-cross-origin.enabled: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.keytab: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.leveldb-state-store.path: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.leveldb-timeline-store.path: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.leveldb-timeline-store.read-cache-size: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.reader.class: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.reader.webapp.address: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.reader.webapp.https.address: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.recovery.enabled: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.state-store-class: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.store-class: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.timeline-client.number-of-async-entities-to-merge: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.ttl-enable: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.ttl-ms: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.version: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.webapp.address: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.webapp.https.address: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.webapp.rest-csrf.custom-header: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.webapp.rest-csrf.enabled: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.webapp.rest-csrf.methods-to-ignore: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.webapp.xfs-filter.xframe-options: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.writer.async.queue.capacity: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.writer.class: string (nullable = true)\n",
      " |    |-- yarn.timeline-service.writer.flush-interval-seconds: string (nullable = true)\n",
      " |    |-- yarn.webapp.api-service.enable: string (nullable = true)\n",
      " |    |-- yarn.webapp.enable-rest-app-submissions: string (nullable = true)\n",
      " |    |-- yarn.webapp.filter-entity-list-by-user: string (nullable = true)\n",
      " |    |-- yarn.webapp.filter-invalid-xml-chars: string (nullable = true)\n",
      " |    |-- yarn.webapp.ui2.enable: string (nullable = true)\n",
      " |    |-- yarn.webapp.xfs-filter.enabled: string (nullable = true)\n",
      " |    |-- yarn.workflow-id.tag-prefix: string (nullable = true)\n",
      " |-- JVM Information: struct (nullable = true)\n",
      " |    |-- Java Home: string (nullable = true)\n",
      " |    |-- Java Version: string (nullable = true)\n",
      " |    |-- Scala Version: string (nullable = true)\n",
      " |-- Job ID: long (nullable = true)\n",
      " |-- Job Result: struct (nullable = true)\n",
      " |    |-- Result: string (nullable = true)\n",
      " |-- Maximum Memory: long (nullable = true)\n",
      " |-- Maximum Offheap Memory: long (nullable = true)\n",
      " |-- Maximum Onheap Memory: long (nullable = true)\n",
      " |-- Metrics Properties: struct (nullable = true)\n",
      " |    |-- *.sink.servlet.class: string (nullable = true)\n",
      " |    |-- *.sink.servlet.path: string (nullable = true)\n",
      " |    |-- applications.sink.servlet.path: string (nullable = true)\n",
      " |    |-- master.sink.servlet.path: string (nullable = true)\n",
      " |-- Properties: struct (nullable = true)\n",
      " |    |-- __fetch_continuous_blocks_in_batch_enabled: string (nullable = true)\n",
      " |    |-- resource.executor.cores: string (nullable = true)\n",
      " |    |-- spark.app.id: string (nullable = true)\n",
      " |    |-- spark.app.initial.jar.urls: string (nullable = true)\n",
      " |    |-- spark.app.name: string (nullable = true)\n",
      " |    |-- spark.app.startTime: string (nullable = true)\n",
      " |    |-- spark.app.submitTime: string (nullable = true)\n",
      " |    |-- spark.databricks.delta.clusteredTable.enableClusteringTablePreview: string (nullable = true)\n",
      " |    |-- spark.delta.logStore.class: string (nullable = true)\n",
      " |    |-- spark.driver.extraJavaOptions: string (nullable = true)\n",
      " |    |-- spark.driver.host: string (nullable = true)\n",
      " |    |-- spark.driver.maxResultSize: string (nullable = true)\n",
      " |    |-- spark.driver.memory: string (nullable = true)\n",
      " |    |-- spark.driver.port: string (nullable = true)\n",
      " |    |-- spark.eventLog.dir: string (nullable = true)\n",
      " |    |-- spark.eventLog.enabled: string (nullable = true)\n",
      " |    |-- spark.executor.cores: string (nullable = true)\n",
      " |    |-- spark.executor.extraJavaOptions: string (nullable = true)\n",
      " |    |-- spark.executor.id: string (nullable = true)\n",
      " |    |-- spark.executor.memory: string (nullable = true)\n",
      " |    |-- spark.hadoop.fs.defaultFS: string (nullable = true)\n",
      " |    |-- spark.hadoop.hive.metastore.uris: string (nullable = true)\n",
      " |    |-- spark.hadoop.javax.jdo.option.ConnectionDriverName: string (nullable = true)\n",
      " |    |-- spark.hadoop.javax.jdo.option.ConnectionPassword: string (nullable = true)\n",
      " |    |-- spark.hadoop.javax.jdo.option.ConnectionURL: string (nullable = true)\n",
      " |    |-- spark.hadoop.javax.jdo.option.ConnectionUserName: string (nullable = true)\n",
      " |    |-- spark.history.fs.logDirectory: string (nullable = true)\n",
      " |    |-- spark.history.fs.update.interval: string (nullable = true)\n",
      " |    |-- spark.history.provider: string (nullable = true)\n",
      " |    |-- spark.history.ui.port: string (nullable = true)\n",
      " |    |-- spark.jars: string (nullable = true)\n",
      " |    |-- spark.jars.packages: string (nullable = true)\n",
      " |    |-- spark.job.description: string (nullable = true)\n",
      " |    |-- spark.job.interruptOnCancel: string (nullable = true)\n",
      " |    |-- spark.job.tags: string (nullable = true)\n",
      " |    |-- spark.master: string (nullable = true)\n",
      " |    |-- spark.rdd.scope: string (nullable = true)\n",
      " |    |-- spark.rdd.scope.noOverride: string (nullable = true)\n",
      " |    |-- spark.repl.class.outputDir: string (nullable = true)\n",
      " |    |-- spark.repl.class.uri: string (nullable = true)\n",
      " |    |-- spark.sql.adaptive.enabled: string (nullable = true)\n",
      " |    |-- spark.sql.catalog.spark_catalog: string (nullable = true)\n",
      " |    |-- spark.sql.catalogImplementation: string (nullable = true)\n",
      " |    |-- spark.sql.execution.id: string (nullable = true)\n",
      " |    |-- spark.sql.execution.root.id: string (nullable = true)\n",
      " |    |-- spark.sql.extensions: string (nullable = true)\n",
      " |    |-- spark.sql.parquet.fieldId.read.enabled: string (nullable = true)\n",
      " |    |-- spark.sql.parquet.fieldId.write.enabled: string (nullable = true)\n",
      " |    |-- spark.sql.shuffle.partitions: string (nullable = true)\n",
      " |    |-- spark.sql.warehouse.dir: string (nullable = true)\n",
      " |    |-- spark.submit.deployMode: string (nullable = true)\n",
      " |    |-- spark.submit.pyFiles: string (nullable = true)\n",
      " |-- Removed Reason: string (nullable = true)\n",
      " |-- Resource Profile Id: long (nullable = true)\n",
      " |-- Spark Properties: struct (nullable = true)\n",
      " |    |-- spark.app.id: string (nullable = true)\n",
      " |    |-- spark.app.initial.jar.urls: string (nullable = true)\n",
      " |    |-- spark.app.name: string (nullable = true)\n",
      " |    |-- spark.app.startTime: string (nullable = true)\n",
      " |    |-- spark.app.submitTime: string (nullable = true)\n",
      " |    |-- spark.databricks.delta.clusteredTable.enableClusteringTablePreview: string (nullable = true)\n",
      " |    |-- spark.delta.logStore.class: string (nullable = true)\n",
      " |    |-- spark.driver.extraJavaOptions: string (nullable = true)\n",
      " |    |-- spark.driver.host: string (nullable = true)\n",
      " |    |-- spark.driver.maxResultSize: string (nullable = true)\n",
      " |    |-- spark.driver.memory: string (nullable = true)\n",
      " |    |-- spark.driver.port: string (nullable = true)\n",
      " |    |-- spark.eventLog.dir: string (nullable = true)\n",
      " |    |-- spark.eventLog.enabled: string (nullable = true)\n",
      " |    |-- spark.executor.cores: string (nullable = true)\n",
      " |    |-- spark.executor.extraJavaOptions: string (nullable = true)\n",
      " |    |-- spark.executor.id: string (nullable = true)\n",
      " |    |-- spark.executor.memory: string (nullable = true)\n",
      " |    |-- spark.hadoop.fs.defaultFS: string (nullable = true)\n",
      " |    |-- spark.hadoop.hive.metastore.uris: string (nullable = true)\n",
      " |    |-- spark.hadoop.javax.jdo.option.ConnectionDriverName: string (nullable = true)\n",
      " |    |-- spark.hadoop.javax.jdo.option.ConnectionPassword: string (nullable = true)\n",
      " |    |-- spark.hadoop.javax.jdo.option.ConnectionURL: string (nullable = true)\n",
      " |    |-- spark.hadoop.javax.jdo.option.ConnectionUserName: string (nullable = true)\n",
      " |    |-- spark.history.fs.logDirectory: string (nullable = true)\n",
      " |    |-- spark.history.fs.update.interval: string (nullable = true)\n",
      " |    |-- spark.history.provider: string (nullable = true)\n",
      " |    |-- spark.history.ui.port: string (nullable = true)\n",
      " |    |-- spark.jars: string (nullable = true)\n",
      " |    |-- spark.jars.packages: string (nullable = true)\n",
      " |    |-- spark.master: string (nullable = true)\n",
      " |    |-- spark.repl.class.outputDir: string (nullable = true)\n",
      " |    |-- spark.repl.class.uri: string (nullable = true)\n",
      " |    |-- spark.scheduler.mode: string (nullable = true)\n",
      " |    |-- spark.sql.adaptive.enabled: string (nullable = true)\n",
      " |    |-- spark.sql.catalog.spark_catalog: string (nullable = true)\n",
      " |    |-- spark.sql.catalogImplementation: string (nullable = true)\n",
      " |    |-- spark.sql.extensions: string (nullable = true)\n",
      " |    |-- spark.sql.shuffle.partitions: string (nullable = true)\n",
      " |    |-- spark.submit.deployMode: string (nullable = true)\n",
      " |    |-- spark.submit.pyFiles: string (nullable = true)\n",
      " |-- Spark Version: string (nullable = true)\n",
      " |-- Stage Attempt ID: long (nullable = true)\n",
      " |-- Stage ID: long (nullable = true)\n",
      " |-- Stage IDs: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- Stage Info: struct (nullable = true)\n",
      " |    |-- Accumulables: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- Count Failed Values: boolean (nullable = true)\n",
      " |    |    |    |-- ID: long (nullable = true)\n",
      " |    |    |    |-- Internal: boolean (nullable = true)\n",
      " |    |    |    |-- Metadata: string (nullable = true)\n",
      " |    |    |    |-- Name: string (nullable = true)\n",
      " |    |    |    |-- Value: string (nullable = true)\n",
      " |    |-- Completion Time: long (nullable = true)\n",
      " |    |-- Details: string (nullable = true)\n",
      " |    |-- Number of Tasks: long (nullable = true)\n",
      " |    |-- Parent IDs: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- RDD Info: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- Barrier: boolean (nullable = true)\n",
      " |    |    |    |-- Callsite: string (nullable = true)\n",
      " |    |    |    |-- DeterministicLevel: string (nullable = true)\n",
      " |    |    |    |-- Disk Size: long (nullable = true)\n",
      " |    |    |    |-- Memory Size: long (nullable = true)\n",
      " |    |    |    |-- Name: string (nullable = true)\n",
      " |    |    |    |-- Number of Cached Partitions: long (nullable = true)\n",
      " |    |    |    |-- Number of Partitions: long (nullable = true)\n",
      " |    |    |    |-- Parent IDs: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- RDD ID: long (nullable = true)\n",
      " |    |    |    |-- Scope: string (nullable = true)\n",
      " |    |    |    |-- Storage Level: struct (nullable = true)\n",
      " |    |    |    |    |-- Deserialized: boolean (nullable = true)\n",
      " |    |    |    |    |-- Replication: long (nullable = true)\n",
      " |    |    |    |    |-- Use Disk: boolean (nullable = true)\n",
      " |    |    |    |    |-- Use Memory: boolean (nullable = true)\n",
      " |    |    |    |    |-- Use Off Heap: boolean (nullable = true)\n",
      " |    |-- Resource Profile Id: long (nullable = true)\n",
      " |    |-- Shuffle Push Enabled: boolean (nullable = true)\n",
      " |    |-- Shuffle Push Mergers Count: long (nullable = true)\n",
      " |    |-- Stage Attempt ID: long (nullable = true)\n",
      " |    |-- Stage ID: long (nullable = true)\n",
      " |    |-- Stage Name: string (nullable = true)\n",
      " |    |-- Submission Time: long (nullable = true)\n",
      " |-- Stage Infos: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- Accumulables: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- Details: string (nullable = true)\n",
      " |    |    |-- Number of Tasks: long (nullable = true)\n",
      " |    |    |-- Parent IDs: array (nullable = true)\n",
      " |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- RDD Info: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- Barrier: boolean (nullable = true)\n",
      " |    |    |    |    |-- Callsite: string (nullable = true)\n",
      " |    |    |    |    |-- DeterministicLevel: string (nullable = true)\n",
      " |    |    |    |    |-- Disk Size: long (nullable = true)\n",
      " |    |    |    |    |-- Memory Size: long (nullable = true)\n",
      " |    |    |    |    |-- Name: string (nullable = true)\n",
      " |    |    |    |    |-- Number of Cached Partitions: long (nullable = true)\n",
      " |    |    |    |    |-- Number of Partitions: long (nullable = true)\n",
      " |    |    |    |    |-- Parent IDs: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |-- RDD ID: long (nullable = true)\n",
      " |    |    |    |    |-- Scope: string (nullable = true)\n",
      " |    |    |    |    |-- Storage Level: struct (nullable = true)\n",
      " |    |    |    |    |    |-- Deserialized: boolean (nullable = true)\n",
      " |    |    |    |    |    |-- Replication: long (nullable = true)\n",
      " |    |    |    |    |    |-- Use Disk: boolean (nullable = true)\n",
      " |    |    |    |    |    |-- Use Memory: boolean (nullable = true)\n",
      " |    |    |    |    |    |-- Use Off Heap: boolean (nullable = true)\n",
      " |    |    |-- Resource Profile Id: long (nullable = true)\n",
      " |    |    |-- Shuffle Push Enabled: boolean (nullable = true)\n",
      " |    |    |-- Shuffle Push Mergers Count: long (nullable = true)\n",
      " |    |    |-- Stage Attempt ID: long (nullable = true)\n",
      " |    |    |-- Stage ID: long (nullable = true)\n",
      " |    |    |-- Stage Name: string (nullable = true)\n",
      " |-- Submission Time: long (nullable = true)\n",
      " |-- System Properties: struct (nullable = true)\n",
      " |    |-- SPARK_SUBMIT: string (nullable = true)\n",
      " |    |-- awt.toolkit: string (nullable = true)\n",
      " |    |-- file.encoding: string (nullable = true)\n",
      " |    |-- file.encoding.pkg: string (nullable = true)\n",
      " |    |-- file.separator: string (nullable = true)\n",
      " |    |-- java.awt.graphicsenv: string (nullable = true)\n",
      " |    |-- java.awt.printerjob: string (nullable = true)\n",
      " |    |-- java.class.version: string (nullable = true)\n",
      " |    |-- java.endorsed.dirs: string (nullable = true)\n",
      " |    |-- java.ext.dirs: string (nullable = true)\n",
      " |    |-- java.home: string (nullable = true)\n",
      " |    |-- java.io.tmpdir: string (nullable = true)\n",
      " |    |-- java.library.path: string (nullable = true)\n",
      " |    |-- java.runtime.name: string (nullable = true)\n",
      " |    |-- java.runtime.version: string (nullable = true)\n",
      " |    |-- java.specification.maintenance.version: string (nullable = true)\n",
      " |    |-- java.specification.name: string (nullable = true)\n",
      " |    |-- java.specification.vendor: string (nullable = true)\n",
      " |    |-- java.specification.version: string (nullable = true)\n",
      " |    |-- java.vendor: string (nullable = true)\n",
      " |    |-- java.vendor.url: string (nullable = true)\n",
      " |    |-- java.vendor.url.bug: string (nullable = true)\n",
      " |    |-- java.version: string (nullable = true)\n",
      " |    |-- java.vm.info: string (nullable = true)\n",
      " |    |-- java.vm.name: string (nullable = true)\n",
      " |    |-- java.vm.specification.name: string (nullable = true)\n",
      " |    |-- java.vm.specification.vendor: string (nullable = true)\n",
      " |    |-- java.vm.specification.version: string (nullable = true)\n",
      " |    |-- java.vm.vendor: string (nullable = true)\n",
      " |    |-- java.vm.version: string (nullable = true)\n",
      " |    |-- jdk.reflect.useDirectMethodHandle: string (nullable = true)\n",
      " |    |-- jetty.git.hash: string (nullable = true)\n",
      " |    |-- line.separator: string (nullable = true)\n",
      " |    |-- os.arch: string (nullable = true)\n",
      " |    |-- os.name: string (nullable = true)\n",
      " |    |-- os.version: string (nullable = true)\n",
      " |    |-- path.separator: string (nullable = true)\n",
      " |    |-- sun.arch.data.model: string (nullable = true)\n",
      " |    |-- sun.boot.class.path: string (nullable = true)\n",
      " |    |-- sun.boot.library.path: string (nullable = true)\n",
      " |    |-- sun.cpu.endian: string (nullable = true)\n",
      " |    |-- sun.cpu.isalist: string (nullable = true)\n",
      " |    |-- sun.io.unicode.encoding: string (nullable = true)\n",
      " |    |-- sun.java.command: string (nullable = true)\n",
      " |    |-- sun.java.launcher: string (nullable = true)\n",
      " |    |-- sun.jnu.encoding: string (nullable = true)\n",
      " |    |-- sun.management.compiler: string (nullable = true)\n",
      " |    |-- sun.os.patch.level: string (nullable = true)\n",
      " |    |-- user.country: string (nullable = true)\n",
      " |    |-- user.dir: string (nullable = true)\n",
      " |    |-- user.home: string (nullable = true)\n",
      " |    |-- user.language: string (nullable = true)\n",
      " |    |-- user.name: string (nullable = true)\n",
      " |    |-- user.timezone: string (nullable = true)\n",
      " |-- Task End Reason: struct (nullable = true)\n",
      " |    |-- Reason: string (nullable = true)\n",
      " |-- Task Executor Metrics: struct (nullable = true)\n",
      " |    |-- DirectPoolMemory: long (nullable = true)\n",
      " |    |-- JVMHeapMemory: long (nullable = true)\n",
      " |    |-- JVMOffHeapMemory: long (nullable = true)\n",
      " |    |-- MajorGCCount: long (nullable = true)\n",
      " |    |-- MajorGCTime: long (nullable = true)\n",
      " |    |-- MappedPoolMemory: long (nullable = true)\n",
      " |    |-- MinorGCCount: long (nullable = true)\n",
      " |    |-- MinorGCTime: long (nullable = true)\n",
      " |    |-- OffHeapExecutionMemory: long (nullable = true)\n",
      " |    |-- OffHeapStorageMemory: long (nullable = true)\n",
      " |    |-- OffHeapUnifiedMemory: long (nullable = true)\n",
      " |    |-- OnHeapExecutionMemory: long (nullable = true)\n",
      " |    |-- OnHeapStorageMemory: long (nullable = true)\n",
      " |    |-- OnHeapUnifiedMemory: long (nullable = true)\n",
      " |    |-- ProcessTreeJVMRSSMemory: long (nullable = true)\n",
      " |    |-- ProcessTreeJVMVMemory: long (nullable = true)\n",
      " |    |-- ProcessTreeOtherRSSMemory: long (nullable = true)\n",
      " |    |-- ProcessTreeOtherVMemory: long (nullable = true)\n",
      " |    |-- ProcessTreePythonRSSMemory: long (nullable = true)\n",
      " |    |-- ProcessTreePythonVMemory: long (nullable = true)\n",
      " |    |-- TotalGCTime: long (nullable = true)\n",
      " |-- Task Info: struct (nullable = true)\n",
      " |    |-- Accumulables: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- Count Failed Values: boolean (nullable = true)\n",
      " |    |    |    |-- ID: long (nullable = true)\n",
      " |    |    |    |-- Internal: boolean (nullable = true)\n",
      " |    |    |    |-- Metadata: string (nullable = true)\n",
      " |    |    |    |-- Name: string (nullable = true)\n",
      " |    |    |    |-- Update: string (nullable = true)\n",
      " |    |    |    |-- Value: string (nullable = true)\n",
      " |    |-- Attempt: long (nullable = true)\n",
      " |    |-- Executor ID: string (nullable = true)\n",
      " |    |-- Failed: boolean (nullable = true)\n",
      " |    |-- Finish Time: long (nullable = true)\n",
      " |    |-- Getting Result Time: long (nullable = true)\n",
      " |    |-- Host: string (nullable = true)\n",
      " |    |-- Index: long (nullable = true)\n",
      " |    |-- Killed: boolean (nullable = true)\n",
      " |    |-- Launch Time: long (nullable = true)\n",
      " |    |-- Locality: string (nullable = true)\n",
      " |    |-- Partition ID: long (nullable = true)\n",
      " |    |-- Speculative: boolean (nullable = true)\n",
      " |    |-- Task ID: long (nullable = true)\n",
      " |-- Task Metrics: struct (nullable = true)\n",
      " |    |-- Disk Bytes Spilled: long (nullable = true)\n",
      " |    |-- Executor CPU Time: long (nullable = true)\n",
      " |    |-- Executor Deserialize CPU Time: long (nullable = true)\n",
      " |    |-- Executor Deserialize Time: long (nullable = true)\n",
      " |    |-- Executor Run Time: long (nullable = true)\n",
      " |    |-- Input Metrics: struct (nullable = true)\n",
      " |    |    |-- Bytes Read: long (nullable = true)\n",
      " |    |    |-- Records Read: long (nullable = true)\n",
      " |    |-- JVM GC Time: long (nullable = true)\n",
      " |    |-- Memory Bytes Spilled: long (nullable = true)\n",
      " |    |-- Output Metrics: struct (nullable = true)\n",
      " |    |    |-- Bytes Written: long (nullable = true)\n",
      " |    |    |-- Records Written: long (nullable = true)\n",
      " |    |-- Peak Execution Memory: long (nullable = true)\n",
      " |    |-- Result Serialization Time: long (nullable = true)\n",
      " |    |-- Result Size: long (nullable = true)\n",
      " |    |-- Shuffle Read Metrics: struct (nullable = true)\n",
      " |    |    |-- Fetch Wait Time: long (nullable = true)\n",
      " |    |    |-- Local Blocks Fetched: long (nullable = true)\n",
      " |    |    |-- Local Bytes Read: long (nullable = true)\n",
      " |    |    |-- Push Based Shuffle: struct (nullable = true)\n",
      " |    |    |    |-- Corrupt Merged Block Chunks: long (nullable = true)\n",
      " |    |    |    |-- Merged Fetch Fallback Count: long (nullable = true)\n",
      " |    |    |    |-- Merged Local Blocks Fetched: long (nullable = true)\n",
      " |    |    |    |-- Merged Local Bytes Read: long (nullable = true)\n",
      " |    |    |    |-- Merged Local Chunks Fetched: long (nullable = true)\n",
      " |    |    |    |-- Merged Remote Blocks Fetched: long (nullable = true)\n",
      " |    |    |    |-- Merged Remote Bytes Read: long (nullable = true)\n",
      " |    |    |    |-- Merged Remote Chunks Fetched: long (nullable = true)\n",
      " |    |    |    |-- Merged Remote Requests Duration: long (nullable = true)\n",
      " |    |    |-- Remote Blocks Fetched: long (nullable = true)\n",
      " |    |    |-- Remote Bytes Read: long (nullable = true)\n",
      " |    |    |-- Remote Bytes Read To Disk: long (nullable = true)\n",
      " |    |    |-- Remote Requests Duration: long (nullable = true)\n",
      " |    |    |-- Total Records Read: long (nullable = true)\n",
      " |    |-- Shuffle Write Metrics: struct (nullable = true)\n",
      " |    |    |-- Shuffle Bytes Written: long (nullable = true)\n",
      " |    |    |-- Shuffle Records Written: long (nullable = true)\n",
      " |    |    |-- Shuffle Write Time: long (nullable = true)\n",
      " |    |-- Updated Blocks: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- Task Resource Requests: struct (nullable = true)\n",
      " |    |-- cpus: struct (nullable = true)\n",
      " |    |    |-- Amount: double (nullable = true)\n",
      " |    |    |-- Resource Name: string (nullable = true)\n",
      " |-- Task Type: string (nullable = true)\n",
      " |-- Timestamp: long (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- accumUpdates: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- details: string (nullable = true)\n",
      " |-- errorMessage: string (nullable = true)\n",
      " |-- executionId: long (nullable = true)\n",
      " |-- jobTags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- modifiedConfigs: struct (nullable = true)\n",
      " |    |-- spark.app.name: string (nullable = true)\n",
      " |    |-- spark.sql.parquet.fieldId.read.enabled: string (nullable = true)\n",
      " |    |-- spark.sql.parquet.fieldId.write.enabled: string (nullable = true)\n",
      " |-- physicalPlanDescription: string (nullable = true)\n",
      " |-- rootExecutionId: long (nullable = true)\n",
      " |-- sparkPlanInfo: struct (nullable = true)\n",
      " |    |-- children: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metadata: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Batched: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- DataFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Format: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Location: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PartitionFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PushedFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- ReadSchema: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metadata: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Batched: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- DataFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Format: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Location: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PartitionFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PushedFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- ReadSchema: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metadata: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Batched: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- DataFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Format: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Location: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PartitionFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PushedFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- ReadSchema: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metadata: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Batched: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- DataFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Format: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Location: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PartitionFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PushedFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- ReadSchema: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metadata: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Batched: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- DataFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Format: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Location: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PartitionFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PushedFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- ReadSchema: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metadata: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Batched: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- DataFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Format: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Location: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PartitionFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PushedFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- ReadSchema: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metadata: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Batched: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- DataFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Format: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Location: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PartitionFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PushedFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- ReadSchema: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metadata: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Batched: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- DataFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Format: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Location: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PartitionFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PushedFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- ReadSchema: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metadata: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Batched: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- DataFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Format: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Location: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PartitionFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PushedFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- ReadSchema: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metadata: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Batched: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- DataFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Format: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Location: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PartitionFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PushedFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- ReadSchema: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metadata: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Batched: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- DataFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Format: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Location: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PartitionFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PushedFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- ReadSchema: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metadata: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Batched: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- DataFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Format: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Location: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PartitionFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PushedFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- ReadSchema: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metadata: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Batched: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- DataFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Format: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Location: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PartitionFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PushedFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- ReadSchema: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metadata: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Batched: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- DataFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Format: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Location: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PartitionFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PushedFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- ReadSchema: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metadata: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Batched: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- DataFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Format: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Location: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PartitionFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PushedFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- ReadSchema: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metadata: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Batched: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- DataFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Format: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Location: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PartitionFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PushedFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- ReadSchema: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metadata: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Batched: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- DataFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Format: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- Location: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PartitionFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- PushedFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- ReadSchema: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- metadata: struct (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |-- Batched: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |-- DataFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |-- Format: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |-- Location: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |-- PartitionFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |-- PushedFilters: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |-- ReadSchema: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |    |    |-- metrics: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- nodeName: string (nullable = true)\n",
      " |    |    |    |-- simpleString: string (nullable = true)\n",
      " |    |-- metrics: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- accumulatorId: long (nullable = true)\n",
      " |    |    |    |-- metricType: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |-- nodeName: string (nullable = true)\n",
      " |    |-- simpleString: string (nullable = true)\n",
      " |-- time: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+---------------+--------------------+-----------+--------------------+--------------------------+--------------------+--------------------+------+----------+--------------+----------------------+---------------------+--------------------+----------+--------------------+-------------------+--------------------+-------------+----------------+--------+---------+----------+-----------+---------------+--------------------+---------------+---------------------+---------+------------+----------------------+---------+-------------+----+------------+-----------+-------+------------+-----------+-------+---------------+-----------------------+---------------+-------------+----+\n",
      "|              App ID|  App Name|    Block Manager ID|   Classpath Entries|Completion Time|               Event|Executor ID|       Executor Info|Executor Resource Requests|   Hadoop Properties|     JVM Information|Job ID|Job Result|Maximum Memory|Maximum Offheap Memory|Maximum Onheap Memory|  Metrics Properties|Properties|      Removed Reason|Resource Profile Id|    Spark Properties|Spark Version|Stage Attempt ID|Stage ID|Stage IDs|Stage Info|Stage Infos|Submission Time|   System Properties|Task End Reason|Task Executor Metrics|Task Info|Task Metrics|Task Resource Requests|Task Type|    Timestamp|User|accumUpdates|description|details|errorMessage|executionId|jobTags|modifiedConfigs|physicalPlanDescription|rootExecutionId|sparkPlanInfo|time|\n",
      "+--------------------+----------+--------------------+--------------------+---------------+--------------------+-----------+--------------------+--------------------------+--------------------+--------------------+------+----------+--------------+----------------------+---------------------+--------------------+----------+--------------------+-------------------+--------------------+-------------+----------------+--------+---------+----------+-----------+---------------+--------------------+---------------+---------------------+---------+------------+----------------------+---------+-------------+----+------------+-----------+-------+------------+-----------+-------+---------------+-----------------------+---------------+-------------+----+\n",
      "|                NULL|      NULL|                NULL|                NULL|           NULL|SparkListenerLogS...|       NULL|                NULL|                      NULL|                NULL|                NULL|  NULL|      NULL|          NULL|                  NULL|                 NULL|                NULL|      NULL|                NULL|               NULL|                NULL|        3.5.1|            NULL|    NULL|     NULL|      NULL|       NULL|           NULL|                NULL|           NULL|                 NULL|     NULL|        NULL|                  NULL|     NULL|         NULL|NULL|        NULL|       NULL|   NULL|        NULL|       NULL|   NULL|           NULL|                   NULL|           NULL|         NULL|NULL|\n",
      "|                NULL|      NULL|                NULL|                NULL|           NULL|SparkListenerReso...|       NULL|                NULL|      {{3, , cores, }, ...|                NULL|                NULL|  NULL|      NULL|          NULL|                  NULL|                 NULL|                NULL|      NULL|                NULL|                  0|                NULL|         NULL|            NULL|    NULL|     NULL|      NULL|       NULL|           NULL|                NULL|           NULL|                 NULL|     NULL|        NULL|         {{1.0, cpus}}|     NULL|         NULL|NULL|        NULL|       NULL|   NULL|        NULL|       NULL|   NULL|           NULL|                   NULL|           NULL|         NULL|NULL|\n",
      "|                NULL|      NULL| {driver, lh, 34793}|                NULL|           NULL|SparkListenerBloc...|       NULL|                NULL|                      NULL|                NULL|                NULL|  NULL|      NULL|     384093388|                     0|            384093388|                NULL|      NULL|                NULL|               NULL|                NULL|         NULL|            NULL|    NULL|     NULL|      NULL|       NULL|           NULL|                NULL|           NULL|                 NULL|     NULL|        NULL|                  NULL|     NULL|1722359501910|NULL|        NULL|       NULL|   NULL|        NULL|       NULL|   NULL|           NULL|                   NULL|           NULL|         NULL|NULL|\n",
      "|                NULL|      NULL|                NULL|{System Classpath...|           NULL|SparkListenerEnvi...|       NULL|                NULL|                      NULL|{false, -1, false...|{/usr/lib/jvm/jav...|  NULL|      NULL|          NULL|                  NULL|                 NULL|{org.apache.spark...|      NULL|                NULL|               NULL|{app-202407301711...|         NULL|            NULL|    NULL|     NULL|      NULL|       NULL|           NULL|{true, sun.awt.X1...|           NULL|                 NULL|     NULL|        NULL|                  NULL|     NULL|         NULL|NULL|        NULL|       NULL|   NULL|        NULL|       NULL|   NULL|           NULL|                   NULL|           NULL|         NULL|NULL|\n",
      "|app-2024073017114...|MyAppScala|                NULL|                NULL|           NULL|SparkListenerAppl...|       NULL|                NULL|                      NULL|                NULL|                NULL|  NULL|      NULL|          NULL|                  NULL|                 NULL|                NULL|      NULL|                NULL|               NULL|                NULL|         NULL|            NULL|    NULL|     NULL|      NULL|       NULL|           NULL|                NULL|           NULL|                 NULL|     NULL|        NULL|                  NULL|     NULL|1722359501837|  lh|        NULL|       NULL|   NULL|        NULL|       NULL|   NULL|           NULL|                   NULL|           NULL|         NULL|NULL|\n",
      "|                NULL|      NULL|                NULL|                NULL|           NULL|SparkListenerExec...|          4|                NULL|                      NULL|                NULL|                NULL|  NULL|      NULL|          NULL|                  NULL|                 NULL|                NULL|      NULL|Command exited wi...|               NULL|                NULL|         NULL|            NULL|    NULL|     NULL|      NULL|       NULL|           NULL|                NULL|           NULL|                 NULL|     NULL|        NULL|                  NULL|     NULL|1722359503577|NULL|        NULL|       NULL|   NULL|        NULL|       NULL|   NULL|           NULL|                   NULL|           NULL|         NULL|NULL|\n",
      "|                NULL|      NULL|                NULL|                NULL|           NULL|SparkListenerExec...|          3|{192.168.0.147, {...|                      NULL|                NULL|                NULL|  NULL|      NULL|          NULL|                  NULL|                 NULL|                NULL|      NULL|                NULL|               NULL|                NULL|         NULL|            NULL|    NULL|     NULL|      NULL|       NULL|           NULL|                NULL|           NULL|                 NULL|     NULL|        NULL|                  NULL|     NULL|1722359503675|NULL|        NULL|       NULL|   NULL|        NULL|       NULL|   NULL|           NULL|                   NULL|           NULL|         NULL|NULL|\n",
      "|                NULL|      NULL|{3, 192.168.0.147...|                NULL|           NULL|SparkListenerBloc...|       NULL|                NULL|                      NULL|                NULL|                NULL|  NULL|      NULL|    4965217075|                     0|           4965217075|                NULL|      NULL|                NULL|               NULL|                NULL|         NULL|            NULL|    NULL|     NULL|      NULL|       NULL|           NULL|                NULL|           NULL|                 NULL|     NULL|        NULL|                  NULL|     NULL|1722359503765|NULL|        NULL|       NULL|   NULL|        NULL|       NULL|   NULL|           NULL|                   NULL|           NULL|         NULL|NULL|\n",
      "|                NULL|      NULL|                NULL|                NULL|           NULL|SparkListenerExec...|          6|                NULL|                      NULL|                NULL|                NULL|  NULL|      NULL|          NULL|                  NULL|                 NULL|                NULL|      NULL|Command exited wi...|               NULL|                NULL|         NULL|            NULL|    NULL|     NULL|      NULL|       NULL|           NULL|                NULL|           NULL|                 NULL|     NULL|        NULL|                  NULL|     NULL|1722359504990|NULL|        NULL|       NULL|   NULL|        NULL|       NULL|   NULL|           NULL|                   NULL|           NULL|         NULL|NULL|\n",
      "|                NULL|      NULL|                NULL|                NULL|           NULL|SparkListenerExec...|          5|{192.168.0.146, {...|                      NULL|                NULL|                NULL|  NULL|      NULL|          NULL|                  NULL|                 NULL|                NULL|      NULL|                NULL|               NULL|                NULL|         NULL|            NULL|    NULL|     NULL|      NULL|       NULL|           NULL|                NULL|           NULL|                 NULL|     NULL|        NULL|                  NULL|     NULL|1722359505745|NULL|        NULL|       NULL|   NULL|        NULL|       NULL|   NULL|           NULL|                   NULL|           NULL|         NULL|NULL|\n",
      "|                NULL|      NULL|                NULL|                NULL|           NULL|SparkListenerExec...|          0|{192.168.0.149, {...|                      NULL|                NULL|                NULL|  NULL|      NULL|          NULL|                  NULL|                 NULL|                NULL|      NULL|                NULL|               NULL|                NULL|         NULL|            NULL|    NULL|     NULL|      NULL|       NULL|           NULL|                NULL|           NULL|                 NULL|     NULL|        NULL|                  NULL|     NULL|1722359505837|NULL|        NULL|       NULL|   NULL|        NULL|       NULL|   NULL|           NULL|                   NULL|           NULL|         NULL|NULL|\n",
      "|                NULL|      NULL|                NULL|                NULL|           NULL|SparkListenerExec...|          2|{192.168.0.150, {...|                      NULL|                NULL|                NULL|  NULL|      NULL|          NULL|                  NULL|                 NULL|                NULL|      NULL|                NULL|               NULL|                NULL|         NULL|            NULL|    NULL|     NULL|      NULL|       NULL|           NULL|                NULL|           NULL|                 NULL|     NULL|        NULL|                  NULL|     NULL|1722359505862|NULL|        NULL|       NULL|   NULL|        NULL|       NULL|   NULL|           NULL|                   NULL|           NULL|         NULL|NULL|\n",
      "|                NULL|      NULL|                NULL|                NULL|           NULL|SparkListenerExec...|          1|{192.168.0.145, {...|                      NULL|                NULL|                NULL|  NULL|      NULL|          NULL|                  NULL|                 NULL|                NULL|      NULL|                NULL|               NULL|                NULL|         NULL|            NULL|    NULL|     NULL|      NULL|       NULL|           NULL|                NULL|           NULL|                 NULL|     NULL|        NULL|                  NULL|     NULL|1722359505870|NULL|        NULL|       NULL|   NULL|        NULL|       NULL|   NULL|           NULL|                   NULL|           NULL|         NULL|NULL|\n",
      "|                NULL|      NULL|{5, 192.168.0.146...|                NULL|           NULL|SparkListenerBloc...|       NULL|                NULL|                      NULL|                NULL|                NULL|  NULL|      NULL|    4965217075|                     0|           4965217075|                NULL|      NULL|                NULL|               NULL|                NULL|         NULL|            NULL|    NULL|     NULL|      NULL|       NULL|           NULL|                NULL|           NULL|                 NULL|     NULL|        NULL|                  NULL|     NULL|1722359505907|NULL|        NULL|       NULL|   NULL|        NULL|       NULL|   NULL|           NULL|                   NULL|           NULL|         NULL|NULL|\n",
      "|                NULL|      NULL|{0, 192.168.0.149...|                NULL|           NULL|SparkListenerBloc...|       NULL|                NULL|                      NULL|                NULL|                NULL|  NULL|      NULL|    4965217075|                     0|           4965217075|                NULL|      NULL|                NULL|               NULL|                NULL|         NULL|            NULL|    NULL|     NULL|      NULL|       NULL|           NULL|                NULL|           NULL|                 NULL|     NULL|        NULL|                  NULL|     NULL|1722359505993|NULL|        NULL|       NULL|   NULL|        NULL|       NULL|   NULL|           NULL|                   NULL|           NULL|         NULL|NULL|\n",
      "|                NULL|      NULL|{2, 192.168.0.150...|                NULL|           NULL|SparkListenerBloc...|       NULL|                NULL|                      NULL|                NULL|                NULL|  NULL|      NULL|    4965217075|                     0|           4965217075|                NULL|      NULL|                NULL|               NULL|                NULL|         NULL|            NULL|    NULL|     NULL|      NULL|       NULL|           NULL|                NULL|           NULL|                 NULL|     NULL|        NULL|                  NULL|     NULL|1722359506005|NULL|        NULL|       NULL|   NULL|        NULL|       NULL|   NULL|           NULL|                   NULL|           NULL|         NULL|NULL|\n",
      "|                NULL|      NULL|{1, 192.168.0.145...|                NULL|           NULL|SparkListenerBloc...|       NULL|                NULL|                      NULL|                NULL|                NULL|  NULL|      NULL|    4965217075|                     0|           4965217075|                NULL|      NULL|                NULL|               NULL|                NULL|         NULL|            NULL|    NULL|     NULL|      NULL|       NULL|           NULL|                NULL|           NULL|                 NULL|     NULL|        NULL|                  NULL|     NULL|1722359506068|NULL|        NULL|       NULL|   NULL|        NULL|       NULL|   NULL|           NULL|                   NULL|           NULL|         NULL|NULL|\n",
      "|                NULL|      NULL|                NULL|                NULL|           NULL|SparkListenerExec...|          7|                NULL|                      NULL|                NULL|                NULL|  NULL|      NULL|          NULL|                  NULL|                 NULL|                NULL|      NULL|Command exited wi...|               NULL|                NULL|         NULL|            NULL|    NULL|     NULL|      NULL|       NULL|           NULL|                NULL|           NULL|                 NULL|     NULL|        NULL|                  NULL|     NULL|1722359506370|NULL|        NULL|       NULL|   NULL|        NULL|       NULL|   NULL|           NULL|                   NULL|           NULL|         NULL|NULL|\n",
      "|                NULL|      NULL|                NULL|                NULL|           NULL|SparkListenerExec...|          8|                NULL|                      NULL|                NULL|                NULL|  NULL|      NULL|          NULL|                  NULL|                 NULL|                NULL|      NULL|Command exited wi...|               NULL|                NULL|         NULL|            NULL|    NULL|     NULL|      NULL|       NULL|           NULL|                NULL|           NULL|                 NULL|     NULL|        NULL|                  NULL|     NULL|1722359507679|NULL|        NULL|       NULL|   NULL|        NULL|       NULL|   NULL|           NULL|                   NULL|           NULL|         NULL|NULL|\n",
      "|                NULL|      NULL|                NULL|                NULL|           NULL|SparkListenerExec...|          9|                NULL|                      NULL|                NULL|                NULL|  NULL|      NULL|          NULL|                  NULL|                 NULL|                NULL|      NULL|Command exited wi...|               NULL|                NULL|         NULL|            NULL|    NULL|     NULL|      NULL|       NULL|           NULL|                NULL|           NULL|                 NULL|     NULL|        NULL|                  NULL|     NULL|1722359508982|NULL|        NULL|       NULL|   NULL|        NULL|       NULL|   NULL|           NULL|                   NULL|           NULL|         NULL|NULL|\n",
      "+--------------------+----------+--------------------+--------------------+---------------+--------------------+-----------+--------------------+--------------------------+--------------------+--------------------+------+----------+--------------+----------------------+---------------------+--------------------+----------+--------------------+-------------------+--------------------+-------------+----------------+--------+---------+----------+-----------+---------------+--------------------+---------------+---------------------+---------+------------+----------------------+---------+-------------+----+------------+-----------+-------+------------+-----------+-------+---------------+-----------------------+---------------+-------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "log_path = \"hdfs://192.168.0.144:9000/spark/history\"\n",
    "\n",
    "event_log_df = spark.read.json(log_path)\n",
    "\n",
    "event_log_df.printSchema()\n",
    "event_log_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48cdab4b-b75e-4a58-a5b2-91f8f3824573",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.1\n",
      "Master: spark://192.168.0.144:7077\n",
      "App Name: MyApp04\n",
      "Spark Web UI: http://lh:4040\n",
      "Is Spark Context active? True\n",
      "\n",
      "Spark Configuration Settings:\n",
      "spark.app.id: app-20240729150139-0001\n",
      "spark.app.initial.jar.urls: spark://lh:40405/jars/delta-spark_2.12-3.2.0.jar,spark://lh:40405/jars/delta-storage-3.2.0.jar\n",
      "spark.app.name: MyApp04\n",
      "spark.app.startTime: 1722265299187\n",
      "spark.app.submitTime: 1722265228321\n",
      "spark.databricks.delta.clusteredTable.enableClusteringTablePreview: true\n",
      "spark.delta.logStore.class: org.apache.spark.sql.delta.storage.HDFSLogStore\n",
      "spark.driver.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.driver.host: lh\n",
      "spark.driver.maxResultSize: 2g\n",
      "spark.driver.memory: 19g\n",
      "spark.driver.port: 40405\n",
      "spark.executor.cores: 3\n",
      "spark.executor.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.executor.id: driver\n",
      "spark.executor.memory: 9g\n",
      "spark.hadoop.fs.defaultFS: hdfs://192.168.0.144:9000\n",
      "spark.hadoop.hive.metastore.uris: thrift://192.168.0.144:9083\n",
      "spark.hadoop.javax.jdo.option.ConnectionDriverName: com.mysql.cj.jdbc.Driver\n",
      "spark.hadoop.javax.jdo.option.ConnectionPassword: Default_Value\n",
      "spark.hadoop.javax.jdo.option.ConnectionURL: jdbc:mysql://192.168.0.144:3306/metastore_db\n",
      "spark.hadoop.javax.jdo.option.ConnectionUserName: lh\n",
      "spark.jars: /usr/local/spark/jars/delta-storage-3.2.0.jar,/usr/local/spark/jars/delta-spark_2.12-3.2.0.jar\n",
      "spark.master: spark://192.168.0.144:7077\n",
      "spark.rdd.compress: True\n",
      "spark.repl.local.jars: file:///usr/local/spark/jars/delta-storage-3.2.0.jar,file:///usr/local/spark/jars/delta-spark_2.12-3.2.0.jar\n",
      "spark.serializer.objectStreamReset: 100\n",
      "spark.sql.catalog.spark_catalog: org.apache.spark.sql.delta.catalog.DeltaCatalog\n",
      "spark.sql.catalogImplementation: hive\n",
      "spark.sql.extensions: io.delta.sql.DeltaSparkSessionExtension\n",
      "spark.sql.warehouse.dir: file:/home/lh/spark-warehouse\n",
      "spark.submit.deployMode: client\n",
      "spark.submit.pyFiles: \n",
      "spark.ui.showConsoleProgress: true\n",
      "\n",
      "Executor Configuration:\n",
      "Executor Memory: 9g\n",
      "Executor Cores: 3\n",
      "Executor Instances: Not Set\n",
      "Driver Memory: 19g\n",
      "Driver Cores: Not Set\n",
      "\n",
      "System Properties:\n",
      "spark.driver.memory: 19g\n",
      "spark.executor.cores: 3\n",
      "spark.executor.memory: 9g\n",
      "\n",
      "Hive-related Spark Configuration:\n",
      "spark.hadoop.hive.metastore.uris: thrift://192.168.0.144:9083\n",
      "\n",
      "Hive Databases:\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|default  |\n",
      "|raw_data |\n",
      "+---------+\n",
      "\n",
      "\n",
      "Details of 'default' Database:\n",
      "+--------------+---------------------------------------------+\n",
      "|info_name     |info_value                                   |\n",
      "+--------------+---------------------------------------------+\n",
      "|Catalog Name  |spark_catalog                                |\n",
      "|Namespace Name|default                                      |\n",
      "|Comment       |Default Hive database                        |\n",
      "|Location      |hdfs://192.168.0.144:9000/user/hive/warehouse|\n",
      "|Owner         |public                                       |\n",
      "|Properties    |                                             |\n",
      "+--------------+---------------------------------------------+\n",
      "\n",
      "\n",
      "Tables in default Database:\n",
      "+---------+----------------+-----------+\n",
      "|namespace|       tableName|isTemporary|\n",
      "+---------+----------------+-----------+\n",
      "|  default|observation_data|      false|\n",
      "+---------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print configuration details\n",
    "print(\"Spark Version:\", spark.sparkContext.version)\n",
    "print(\"Master:\", spark.sparkContext.master)\n",
    "print(\"App Name:\", spark.sparkContext.appName)\n",
    "print(\"Spark Web UI:\", spark.sparkContext.uiWebUrl)\n",
    "\n",
    "print(\"Is Spark Context active?\", not spark.sparkContext._jsc.sc().isStopped())\n",
    "\n",
    "print(\"\\nSpark Configuration Settings:\")\n",
    "for item in sorted(spark.sparkContext.getConf().getAll()):\n",
    "    print(f\"{item[0]}: {item[1]}\")\n",
    "\n",
    "print(\"\\nExecutor Configuration:\")\n",
    "print(\"Executor Memory:\", spark.conf.get(\"spark.executor.memory\", \"Not Set\"))\n",
    "print(\"Executor Cores:\", spark.conf.get(\"spark.executor.cores\", \"Not Set\"))\n",
    "print(\"Executor Instances:\", spark.conf.get(\"spark.executor.instances\", \"Not Set\"))\n",
    "\n",
    "print(\"Driver Memory:\", spark.conf.get(\"spark.driver.memory\", \"Not Set\"))\n",
    "print(\"Driver Cores:\", spark.conf.get(\"spark.driver.cores\", \"Not Set\"))\n",
    "\n",
    "print(\"\\nSystem Properties:\")\n",
    "for prop in spark.sparkContext.getConf().getAll():\n",
    "    if \"memory\" in prop[0] or \"cores\" in prop[0]:\n",
    "        print(f\"{prop[0]}: {prop[1]}\")\n",
    "\n",
    "print(\"\\nHive-related Spark Configuration:\")\n",
    "for k, v in spark.sparkContext.getConf().getAll():\n",
    "    if \"hive\" in k:\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\nHive Databases:\")\n",
    "spark.sql(\"SHOW DATABASES\").show(truncate=False)\n",
    "\n",
    "print(\"\\nDetails of 'default' Database:\")\n",
    "spark.sql(\"DESCRIBE DATABASE EXTENDED default\").show(truncate=False)\n",
    "\n",
    "print(\"\\nTables in default Database:\")\n",
    "spark.sql(\"SHOW TABLES IN default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f23c8bc-9157-49ad-8900-24cc22646112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databases:\n",
      "+----------------+\n",
      "|       namespace|\n",
      "+----------------+\n",
      "|      compaction|\n",
      "|         default|\n",
      "|liquidclustering|\n",
      "|       partition|\n",
      "|        raw_data|\n",
      "|          stream|\n",
      "|          zorder|\n",
      "+----------------+\n",
      "\n",
      "Tables in 'raw_data' database:\n",
      "+---------+--------------+-----------+\n",
      "|namespace|tableName     |isTemporary|\n",
      "+---------+--------------+-----------+\n",
      "|raw_data |campaign_dim  |false      |\n",
      "|raw_data |customer_dim  |false      |\n",
      "|raw_data |department_dim|false      |\n",
      "|raw_data |location_dim  |false      |\n",
      "|raw_data |product_dim   |false      |\n",
      "|raw_data |raw_data_2    |false      |\n",
      "|raw_data |raw_data_4    |false      |\n",
      "|raw_data |raw_data_8    |false      |\n",
      "|raw_data |rawdata_2     |false      |\n",
      "|raw_data |rawdata_4     |false      |\n",
      "|raw_data |rawdata_8     |false      |\n",
      "+---------+--------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------+---------------------------------+-----------+------------------------------------------------------+----------------------+-----------------------+----------------+-----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "|format|id                                  |name                             |description|location                                              |createdAt             |lastModified           |partitionColumns|clusteringColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|tableFeatures           |\n",
      "+------+------------------------------------+---------------------------------+-----------+------------------------------------------------------+----------------------+-----------------------+----------------+-----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "|delta |ca00d492-cf92-40e9-80ee-6a159e8811cf|spark_catalog.raw_data.raw_data_2|NULL       |hdfs://192.168.0.144:9000/datalake/raw_data/raw_data_2|2024-07-29 15:04:12.15|2024-07-29 16:17:10.386|[]              |[]               |15      |43358996135|{}        |1               |2               |[appendOnly, invariants]|\n",
      "+------+------------------------------------+---------------------------------+-----------+------------------------------------------------------+----------------------+-----------------------+----------------+-----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "\n",
      "+-------------+---------+-------+\n",
      "|col_name     |data_type|comment|\n",
      "+-------------+---------+-------+\n",
      "|timestamp    |timestamp|NULL   |\n",
      "|value        |double   |NULL   |\n",
      "|country      |string   |NULL   |\n",
      "|event_id     |bigint   |NULL   |\n",
      "|actor_id     |bigint   |NULL   |\n",
      "|year         |int      |NULL   |\n",
      "|month        |bigint   |NULL   |\n",
      "|day          |bigint   |NULL   |\n",
      "|product_id   |int      |NULL   |\n",
      "|location_id  |int      |NULL   |\n",
      "|department_id|int      |NULL   |\n",
      "|campaign_id  |int      |NULL   |\n",
      "|customer_id  |int      |NULL   |\n",
      "+-------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Databases:\")\n",
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "spark.sql(\"USE raw_data\")\n",
    "print(\"Tables in 'raw_data' database:\")\n",
    "spark.sql(\"SHOW TABLES\").show(truncate=False)\n",
    "spark.sql(\"DESCRIBE DETAIL raw_data_2\").show(truncate=False)\n",
    "spark.sql(\"DESCRIBE TABLE raw_data_2\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49433fa6-6c83-43b9-a2ae-b873a86691fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/31 16:31:18 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`raw_data`.`product_dim` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "24/07/31 16:31:18 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "24/07/31 16:31:20 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`raw_data`.`location_dim` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "24/07/31 16:31:23 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`raw_data`.`department_dim` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "24/07/31 16:31:24 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`raw_data`.`campaign_dim` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "24/07/31 16:31:25 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`raw_data`.`customer_dim` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    }
   ],
   "source": [
    "# Create dimension tables\n",
    "\n",
    "tables = [\n",
    "    {\n",
    "        \"name\": \"product_dim\",\n",
    "        \"columns\": \"\"\"\n",
    "            product_id INT,\n",
    "            product_name STRING,\n",
    "            product_category STRING,\n",
    "            product_price DOUBLE\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"location_dim\",\n",
    "        \"columns\": \"\"\"\n",
    "            location_id INT,\n",
    "            location_name STRING,\n",
    "            city STRING,\n",
    "            state STRING,\n",
    "            country STRING\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"department_dim\",\n",
    "        \"columns\": \"\"\"\n",
    "            department_id INT,\n",
    "            department_name STRING,\n",
    "            department_head STRING\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"campaign_dim\",\n",
    "        \"columns\": \"\"\"\n",
    "            campaign_id INT,\n",
    "            campaign_name STRING,\n",
    "            start_date DATE,\n",
    "            end_date DATE\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"customer_dim\",\n",
    "        \"columns\": \"\"\"\n",
    "            customer_id INT,\n",
    "            customer_name STRING,\n",
    "            customer_email STRING,\n",
    "            customer_segment STRING\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for table in tables:\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE {table['name']} (\n",
    "        {table['columns']}\n",
    "    )\n",
    "    USING delta;\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bc5e6ca-8340-40cd-a428-79e4bb6cc06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fill dimension tables with data\n",
    "data = {\n",
    "    \"product_dim\": [\n",
    "        (1, 'Product A', 'Category 1', 19.99),\n",
    "        (2, 'Product B', 'Category 2', 29.99),\n",
    "        (3, 'Product C', 'Category 1', 39.99),\n",
    "        (4, 'Product D', 'Category 3', 49.99),\n",
    "        (5, 'Product E', 'Category 2', 59.99),\n",
    "        (6, 'Product F', 'Category 4', 69.99),\n",
    "        (7, 'Product G', 'Category 1', 79.99),\n",
    "        (8, 'Product H', 'Category 5', 89.99),\n",
    "        (9, 'Product I', 'Category 3', 99.99),\n",
    "        (10, 'Product J', 'Category 2', 109.99)\n",
    "    ],\n",
    "    \"location_dim\": [\n",
    "        (1, 'Location A', 'New York', 'NY', 'USA'),\n",
    "        (2, 'Location B', 'Los Angeles', 'CA', 'USA'),\n",
    "        (3, 'Location C', 'Chicago', 'IL', 'USA'),\n",
    "        (4, 'Location D', 'Houston', 'TX', 'USA'),\n",
    "        (5, 'Location E', 'Phoenix', 'AZ', 'USA'),\n",
    "        (6, 'Location F', 'Philadelphia', 'PA', 'USA'),\n",
    "        (7, 'Location G', 'San Antonio', 'TX', 'USA'),\n",
    "        (8, 'Location H', 'San Diego', 'CA', 'USA'),\n",
    "        (9, 'Location I', 'Dallas', 'TX', 'USA'),\n",
    "        (10, 'Location J', 'San Jose', 'CA', 'USA')\n",
    "    ],\n",
    "    \"department_dim\": [\n",
    "        (1, 'Sales', 'Alice Smith'),\n",
    "        (2, 'Marketing', 'Bob Jones'),\n",
    "        (3, 'Engineering', 'Charlie Brown'),\n",
    "        (4, 'HR', 'Diana Green'),\n",
    "        (5, 'Finance', 'Eve White'),\n",
    "        (6, 'Product', 'Frank Black'),\n",
    "        (7, 'Customer Support', 'Grace Blue'),\n",
    "        (8, 'IT', 'Hank Red'),\n",
    "        (9, 'Logistics', 'Ivy Yellow'),\n",
    "        (10, 'Legal', 'Jack Orange')\n",
    "    ],\n",
    "    \"campaign_dim\": [\n",
    "        (1, 'Campaign 1', '2024-01-01', '2024-01-31'),\n",
    "        (2, 'Campaign 2', '2024-02-01', '2024-02-28'),\n",
    "        (3, 'Campaign 3', '2024-03-01', '2024-03-31'),\n",
    "        (4, 'Campaign 4', '2024-04-01', '2024-04-30'),\n",
    "        (5, 'Campaign 5', '2024-05-01', '2024-05-31'),\n",
    "        (6, 'Campaign 6', '2024-06-01', '2024-06-30'),\n",
    "        (7, 'Campaign 7', '2024-07-01', '2024-07-31'),\n",
    "        (8, 'Campaign 8', '2024-08-01', '2024-08-31'),\n",
    "        (9, 'Campaign 9', '2024-09-01', '2024-09-30'),\n",
    "        (10, 'Campaign 10', '2024-10-01', '2024-10-31')\n",
    "    ],\n",
    "    \"customer_dim\": [\n",
    "        (1, 'John Doe', 'john.doe@example.com', 'Premium'),\n",
    "        (2, 'Jane Smith', 'jane.smith@example.com', 'Standard'),\n",
    "        (3, 'Michael Brown', 'michael.brown@example.com', 'Standard'),\n",
    "        (4, 'Emily Davis', 'emily.davis@example.com', 'Premium'),\n",
    "        (5, 'Chris Johnson', 'chris.johnson@example.com', 'Basic'),\n",
    "        (6, 'Olivia Martin', 'olivia.martin@example.com', 'Premium'),\n",
    "        (7, 'Liam Lee', 'liam.lee@example.com', 'Standard'),\n",
    "        (8, 'Sophia Wilson', 'sophia.wilson@example.com', 'Basic'),\n",
    "        (9, 'Jackson Moore', 'jackson.moore@example.com', 'Standard'),\n",
    "        (10, 'Ava Taylor', 'ava.taylor@example.com', 'Premium')\n",
    "    ]\n",
    "}\n",
    "\n",
    "for table, values in data.items():\n",
    "    values_str = \", \".join(str(value) for value in values)\n",
    "    spark.sql(f\"\"\"\n",
    "    INSERT INTO {table} VALUES\n",
    "    {values_str}\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d61baa16-3d6a-48bd-ad66-878b8af64eb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/29 15:04:05 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`raw_data`.`raw_data_8` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "24/07/29 15:04:05 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "24/07/29 15:04:12 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`raw_data`.`raw_data_4` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "24/07/29 15:04:12 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`raw_data`.`raw_data_2` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting data into table: raw_data_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/29 15:04:13 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 525 ms, sys: 469 ms, total: 994 ms\n",
      "Wall time: 39min 47s\n",
      "Inserting data into table: raw_data_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 338 ms, sys: 208 ms, total: 546 ms\n",
      "Wall time: 20min 44s\n",
      "Inserting data into table: raw_data_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 137 ms, sys: 161 ms, total: 298 ms\n",
      "Wall time: 12min 25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create raw data and insert it into the existing Delta Lake tables\n",
    "\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from pyspark.sql.functions import lit, rand, expr, year, month, dayofmonth, concat, lpad, element_at, array\n",
    "from pyspark.sql.types import StringType, StructType, StructField, DateType, TimestampType, DoubleType, LongType, IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "import itertools\n",
    "import random\n",
    "from pyspark.sql.functions import lit, rand, udf, col, expr, year, month, dayofmonth, concat, lpad\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "# Parameters\n",
    "num_tables = 3  # Number of table pairs to create\n",
    "max_rows = 8  # Maximum number of rows for the largest table 8000000000\n",
    "\n",
    "row_counts = [max_rows // (2 ** i) for i in range(num_pairs)]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create and initialize a specified number of partitioned and non-partitioned Delta Lake tables.\n",
    "Each pair of tables (partitioned and non-partitioned) is dropped if it exists and then recreated \n",
    "with the specified schema.\n",
    "\"\"\"\n",
    "for i in range(num_tables):\n",
    "    raw_data = f\"raw_data_{row_counts[i]}\"\n",
    "    \n",
    "    # Drop tables if they exist\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {raw_data}\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE {raw_data} (\n",
    "        timestamp TIMESTAMP,\n",
    "        value DOUBLE,\n",
    "        country STRING,\n",
    "        event_id LONG,\n",
    "        actor_id LONG,\n",
    "        year INT,\n",
    "        month LONG,\n",
    "        day LONG,\n",
    "        product_id INT,          \n",
    "        location_id INT,         \n",
    "        department_id INT,      \n",
    "        campaign_id INT,         \n",
    "        customer_id INT        \n",
    "    )\n",
    "    USING delta;\n",
    "    \"\"\")\n",
    "\n",
    "countries = [\"USA\", \"Canada\", \"Mexico\", \"Brazil\", \"Argentina\", \"UK\", \"France\", \"Germany\", \"Italy\", \"Spain\",\n",
    "             \"China\", \"Japan\", \"South Korea\", \"India\", \"Australia\", \"New Zealand\", \"South Africa\", \"Egypt\", \"Nigeria\", \"Kenya\",\n",
    "             \"Russia\", \"Turkey\", \"Saudi Arabia\", \"Indonesia\", \"Thailand\", \"Malaysia\", \"Philippines\", \"Vietnam\", \"Singapore\", \"Pakistan\",\n",
    "             \"Bangladesh\", \"Sri Lanka\", \"Nepal\", \"Bhutan\", \"Maldives\"]\n",
    "\n",
    "\n",
    "def generate_data(timestamp, nrows):\n",
    "    \"\"\"\n",
    "    Generate a DataFrame with synthetic data based on the given timestamp and number of rows.\n",
    "\n",
    "    This function creates a DataFrame with the specified number of rows (`nrows`), where each row contains:\n",
    "    - A timestamp incremented by 0.1 seconds for each row.\n",
    "    - A random value for the `value` column.\n",
    "    - A random country selected from a predefined list of countries.\n",
    "    - Sequential event and actor IDs.\n",
    "    - Year, month, and day components derived from the timestamp.\n",
    "\n",
    "    Parameters:\n",
    "    timestamp (datetime): The base timestamp used to generate the date and initial timestamp for each row.\n",
    "    nrows (int): The number of rows to generate in the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A Spark DataFrame with the generated data, including columns for date, timestamp, value, country,\n",
    "               event_id, actor_id, year, month, and day.\n",
    "\n",
    "    Example:\n",
    "    >>> from datetime import datetime\n",
    "    >>> df = generate_data(datetime(2024, 6, 1), 1000)\n",
    "    >>> df.show(3)\n",
    "    +--------------------+-------------------+--------+--------+--------+----+------+--------+----------+-----------+-------------+-----------+-----------+\n",
    "|           timestamp|              value| country|event_id|actor_id|year| month|     day|product_id|location_id|department_id|campaign_id|customer_id|\n",
    "+---------------------+-------------------+---------+--------+--------+----+------+--------+----------+-----------+-------------+-----------+-----------+\n",
    "|timestamp            |value              |country  |event_id|actor_id|year|month |day     |product_id|location_id|department_id|campaign_id|customer_id|\n",
    "+---------------------+-------------------+---------+--------+--------+----+------+--------+----------+-----------+-------------+-----------+-----------+\n",
    "|2024-06-03 09:42:13.3|0.6414014977824959 |Australia|212261  |4862    |2024|202406|20240603|4         |4          |4            |4          |4          |\n",
    "|2024-06-03 09:42:13.4|0.7582936836987177 |UK       |351215  |1334    |2024|202406|20240603|5         |5          |5            |5          |5          |\n",
    "|2024-06-03 09:42:13.5|0.44169132871707106|Sri Lanka|886013  |6109    |2024|202406|20240603|6         |6          |6            |6          |6          |\n",
    "+---------------------+-------------------+---------+--------+--------+----+------+--------+----------+-----------+-------------+-----------+-----------+    \"\"\"\n",
    "    date = timestamp.date()\n",
    "    return spark.range(nrows*1000000000).select(\n",
    "        (lit(timestamp) + expr(\"INTERVAL 0.1 SECOND\") * col(\"id\")).alias(\"timestamp\"),\n",
    "        rand().alias(\"value\"),\n",
    "        element_at(array([lit(country) for country in countries]), (rand() * len(countries)).cast(\"int\") + 1).alias(\"country\"),\n",
    "        (rand() * 1000000).cast(\"long\").alias(\"event_id\"),\n",
    "        (rand() * 10000).cast(\"long\").alias(\"actor_id\"),\n",
    "        year((lit(timestamp) + expr(\"INTERVAL 0.1 SECOND\") * col(\"id\"))).alias(\"year\"),\n",
    "        concat(\n",
    "            year((lit(timestamp) + expr(\"INTERVAL 0.1 SECOND\") * col(\"id\"))).cast(\"string\"),\n",
    "            lpad(month((lit(timestamp) + expr(\"INTERVAL 0.1 SECOND\") * col(\"id\"))).cast(\"string\"), 2, '0')\n",
    "        ).cast(\"long\").alias(\"month\"),\n",
    "        concat(\n",
    "            year((lit(timestamp) + expr(\"INTERVAL 0.1 SECOND\") * col(\"id\"))).cast(\"string\"),\n",
    "            lpad(month((lit(timestamp) + expr(\"INTERVAL 0.1 SECOND\") * col(\"id\"))).cast(\"string\"), 2, '0'),\n",
    "            lpad(dayofmonth((lit(timestamp) + expr(\"INTERVAL 0.1 SECOND\") * col(\"id\"))).cast(\"string\"), 2, '0')\n",
    "        ).cast(\"long\").alias(\"day\"),\n",
    "        expr(\"id % 10 + 1\").cast(\"int\").alias(\"product_id\"),       \n",
    "        expr(\"id % 10 + 1\").cast(\"int\").alias(\"location_id\"),      \n",
    "        expr(\"id % 10 + 1\").cast(\"int\").alias(\"department_id\"),    \n",
    "        expr(\"id % 10 + 1\").cast(\"int\").alias(\"campaign_id\"),       \n",
    "        expr(\"id % 10 + 1\").cast(\"int\").alias(\"customer_id\")        \n",
    "    )\n",
    "\n",
    "\n",
    "for i, rows in enumerate(row_counts):\n",
    "    timestamp = datetime(2024, 1, 1) + timedelta(hours=i)\n",
    "    \n",
    "    raw_data = f\"raw_data_{row_counts[i]}\"\n",
    "    \n",
    "    print(f\"Inserting data into table: {raw_data}\")\n",
    "    insert_data_part_table_name = generate_data(timestamp, rows)\n",
    "    %time insert_data_part_table_name.write.format(\"delta\").mode(\"append\").saveAsTable(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "249347e9-e299-41c2-9bb5-1e486b97d369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------------------+---------+--------+--------+----+------+--------+----------+-----------+-------------+-----------+-----------+\n",
      "|timestamp            |value              |country  |event_id|actor_id|year|month |day     |product_id|location_id|department_id|campaign_id|customer_id|\n",
      "+---------------------+-------------------+---------+--------+--------+----+------+--------+----------+-----------+-------------+-----------+-----------+\n",
      "|2024-06-03 09:42:13.3|0.6414014977824959 |Australia|212261  |4862    |2024|202406|20240603|4         |4          |4            |4          |4          |\n",
      "|2024-06-03 09:42:13.4|0.7582936836987177 |UK       |351215  |1334    |2024|202406|20240603|5         |5          |5            |5          |5          |\n",
      "|2024-06-03 09:42:13.5|0.44169132871707106|Sri Lanka|886013  |6109    |2024|202406|20240603|6         |6          |6            |6          |6          |\n",
      "+---------------------+-------------------+---------+--------+--------+----+------+--------+----------+-----------+-------------+-----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from raw_data_2 limit 3\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5018d4c6-c8a6-4c86-b823-3457a5acc43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|namespace| tableName|isTemporary|\n",
      "+---------+----------+-----------+\n",
      "| raw_data|raw_data_2|      false|\n",
      "| raw_data|raw_data_4|      false|\n",
      "| raw_data|raw_data_8|      false|\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"show tables\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03180f07-8532-476b-a963-2487f73308c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display table properties\n",
    "\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def display_table_properties(spark, database, tables):\n",
    "    # Set Hadoop environment variables\n",
    "    os.environ['HADOOP_HOME'] = '/usr/local/hadoop'\n",
    "    os.environ['PATH'] = os.environ['PATH'] + ':' + os.environ['HADOOP_HOME'] + '/bin'\n",
    "\n",
    "    spark.sql(f\"USE {database}\")\n",
    "\n",
    "    table_details = []\n",
    "\n",
    "    for table_name in tables:\n",
    "        describe_df = spark.sql(f\"DESCRIBE DETAIL {table_name}\")\n",
    "        describe_detail = describe_df.collect()[0].asDict()\n",
    "        hdfs_location = describe_detail['location']\n",
    "        num_files = describe_detail['numFiles']\n",
    "        partition_columns = describe_detail['partitionColumns']\n",
    "\n",
    "        result = subprocess.run([\"hdfs\", \"dfs\", \"-du\", \"-s\", hdfs_location], capture_output=True, text=True)\n",
    "        size = int(result.stdout.split()[0])\n",
    "\n",
    "        count_result = subprocess.run([\"hdfs\", \"dfs\", \"-count\", hdfs_location], capture_output=True, text=True)\n",
    "        subdirectories = int(count_result.stdout.split()[0]) - 1  \n",
    "\n",
    "        row_count_df = spark.sql(f\"SELECT COUNT(*) AS row_count FROM {table_name}\")\n",
    "        row_count = row_count_df.collect()[0]['row_count']\n",
    "\n",
    "        table_details.append((table_name, hdfs_location, size, num_files, partition_columns, subdirectories, row_count))\n",
    "\n",
    "    for table in table_details:\n",
    "        print(f\"Table: {table[0]}, HDFS Directory: {table[1]}, Size: {table[2]} bytes, Number of Files: {table[3]}, Partition Columns: {table[4]}, Subdirectories: {table[5]}, Number of Rows: {table[6]}\")\n",
    "\n",
    "    # Extract data for plotting\n",
    "    table_names = [table[0] for table in table_details]\n",
    "    sizes = [table[2] / (1024 ** 3) for table in table_details] \n",
    "    num_files = [table[3] for table in table_details]\n",
    "    subdirectories = [table[5] for table in table_details]\n",
    "    row_counts = [table[6] for table in table_details]\n",
    "\n",
    "    fig, axs = plt.subplots(4, 1, figsize=(6, 12))\n",
    "\n",
    "    axs[0].bar(table_names, sizes, color='b')\n",
    "    axs[0].set_title('Table Sizes')\n",
    "    axs[0].set_ylabel('Size (GB)')\n",
    "    axs[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    axs[1].bar(table_names, num_files, color='g')\n",
    "    axs[1].set_title('Number of Files')\n",
    "    axs[1].set_ylabel('Files')\n",
    "    axs[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    axs[2].bar(table_names, subdirectories, color='r')\n",
    "    axs[2].set_title('Number of Subdirectories')\n",
    "    axs[2].set_ylabel('Subdirectories')\n",
    "    axs[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    axs[3].bar(table_names, row_counts, color='c')\n",
    "    axs[3].set_title('Number of Rows')\n",
    "    axs[3].set_ylabel('Rows')\n",
    "    axs[3].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# spark is the SparkSession\n",
    "# display_table_properties(spark, \"raw_data\", ['raw_data_2', 'raw_data_4', 'raw_data_8'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "badadcec-92e2-47a3-912b-242177df43ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: raw_data_2, HDFS Directory: hdfs://192.168.0.144:9000/datalake/raw_data/raw_data_2, Size: 43359013259 bytes, Number of Files: 15, Partition Columns: [], Subdirectories: 2, Number of Rows: 2000000000\n",
      "Table: raw_data_4, HDFS Directory: hdfs://192.168.0.144:9000/datalake/raw_data/raw_data_4, Size: 86718037909 bytes, Number of Files: 15, Partition Columns: [], Subdirectories: 2, Number of Rows: 4000000000\n",
      "Table: raw_data_8, HDFS Directory: hdfs://192.168.0.144:9000/datalake/raw_data/raw_data_8, Size: 173433056152 bytes, Number of Files: 15, Partition Columns: [], Subdirectories: 2, Number of Rows: 8000000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_table_properties(spark, \"raw_data\", ['raw_data_2', 'raw_data_4', 'raw_data_8'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4793eb26-5087-4f6e-a05c-fa655b70815e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/03 14:56:57 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`raw_data`.`rawdata_2` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "24/08/03 14:56:58 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "24/08/03 14:57:04 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`raw_data`.`rawdata_4` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "24/08/03 14:57:04 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`raw_data`.`rawdata_8` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create another set of tables with different file size\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE rawdata_2 (\n",
    "        timestamp TIMESTAMP,\n",
    "        value DOUBLE,\n",
    "        country STRING,\n",
    "        event_id LONG,\n",
    "        actor_id LONG,\n",
    "        year INT,\n",
    "        month LONG,\n",
    "        day LONG,\n",
    "        product_id INT,          \n",
    "        location_id INT,         \n",
    "        department_id INT,      \n",
    "        campaign_id INT,         \n",
    "        customer_id INT        \n",
    "    )\n",
    "    USING delta;\n",
    "    \"\"\")\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE rawdata_4 (\n",
    "        timestamp TIMESTAMP,\n",
    "        value DOUBLE,\n",
    "        country STRING,\n",
    "        event_id LONG,\n",
    "        actor_id LONG,\n",
    "        year INT,\n",
    "        month LONG,\n",
    "        day LONG,\n",
    "        product_id INT,          \n",
    "        location_id INT,         \n",
    "        department_id INT,      \n",
    "        campaign_id INT,         \n",
    "        customer_id INT        \n",
    "    )\n",
    "    USING delta;\n",
    "    \"\"\")\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE rawdata_8 (\n",
    "        timestamp TIMESTAMP,\n",
    "        value DOUBLE,\n",
    "        country STRING,\n",
    "        event_id LONG,\n",
    "        actor_id LONG,\n",
    "        year INT,\n",
    "        month LONG,\n",
    "        day LONG,\n",
    "        product_id INT,          \n",
    "        location_id INT,         \n",
    "        department_id INT,      \n",
    "        campaign_id INT,         \n",
    "        customer_id INT        \n",
    "    )\n",
    "    USING delta;\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eeca5f0c-9392-45e7-842f-ba4fc876c7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create another set of tables with different file size\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    drop TABLE rawdata_2 \n",
    "    \"\"\")\n",
    "spark.sql(f\"\"\"\n",
    "    drop TABLE rawdata_4 \n",
    "    \"\"\")\n",
    "spark.sql(f\"\"\"\n",
    "    drop TABLE rawdata_8 \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6edd91e2-49cd-4500-9c6e-32435d3cb83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "use database  zorder  \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9bc9fbfe-ade6-4ece-851d-d2efa0830c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    drop TABLE zorder_eventid_table_8\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9f7dac-e10a-40ea-8fae-a623d26c17be",
   "metadata": {},
   "outputs": [],
   "source": [
    "zorder_eventid_table_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "407544fb-5228-4ff3-aebc-374f91c677e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "drop database stream cascade   \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee7b1831-43e5-4423-87f0-d47cd0883339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/05 17:18:46 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`raw_data`.`rawdata_2` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "24/08/05 17:18:46 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "24/08/05 17:18:47 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`raw_data`.`rawdata_4` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "24/08/05 17:18:49 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`raw_data`.`rawdata_8` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create another set of tables with different file size\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE rawdata_2 (\n",
    "        timestamp TIMESTAMP,\n",
    "        value DOUBLE,\n",
    "        country STRING,\n",
    "        event_id LONG,\n",
    "        actor_id LONG,\n",
    "        year INT,\n",
    "        month LONG,\n",
    "        day LONG,\n",
    "        product_id INT,          \n",
    "        location_id INT,         \n",
    "        department_id INT,      \n",
    "        campaign_id INT,         \n",
    "        customer_id INT        \n",
    "    )\n",
    "    USING delta;\n",
    "    \"\"\")\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE rawdata_4 (\n",
    "        timestamp TIMESTAMP,\n",
    "        value DOUBLE,\n",
    "        country STRING,\n",
    "        event_id LONG,\n",
    "        actor_id LONG,\n",
    "        year INT,\n",
    "        month LONG,\n",
    "        day LONG,\n",
    "        product_id INT,          \n",
    "        location_id INT,         \n",
    "        department_id INT,      \n",
    "        campaign_id INT,         \n",
    "        customer_id INT        \n",
    "    )\n",
    "    USING delta;\n",
    "    \"\"\")\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE rawdata_8 (\n",
    "        timestamp TIMESTAMP,\n",
    "        value DOUBLE,\n",
    "        country STRING,\n",
    "        event_id LONG,\n",
    "        actor_id LONG,\n",
    "        year INT,\n",
    "        month LONG,\n",
    "        day LONG,\n",
    "        product_id INT,          \n",
    "        location_id INT,         \n",
    "        department_id INT,      \n",
    "        campaign_id INT,         \n",
    "        customer_id INT        \n",
    "    )\n",
    "    USING delta;\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3cbdca9f-4c98-44cd-a02a-e3f732ee4d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting data for table rawdata_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.           (128 + 15) / 1293]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lh/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/lh/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting data for table rawdata_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/05 18:27:41 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "24/08/05 18:27:41 ERROR DeltaFileFormatWriter: Aborting job 1ab98887-3525-4cf5-826b-008e9083619f.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: KILLED\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.$anonfun$executeWrite$1(DeltaFileFormatWriter.scala:263)\n",
      "\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.writeAndCommit(DeltaFileFormatWriter.scala:295)\n",
      "\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.executeWrite(DeltaFileFormatWriter.scala:234)\n",
      "\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.write(DeltaFileFormatWriter.scala:214)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$writeFiles$1(TransactionalWrite.scala:440)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:398)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:371)\n",
      "\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:147)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:246)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:242)\n",
      "\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:147)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:235)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:232)\n",
      "\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:147)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.writeFiles(WriteIntoDelta.scala:349)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.writeAndReturnCommitData(WriteIntoDelta.scala:312)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1(WriteIntoDelta.scala:106)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1$adapted(WriteIntoDelta.scala:101)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:227)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.run(WriteIntoDelta.scala:101)\n",
      "\tat org.apache.spark.sql.delta.catalog.WriteIntoDeltaBuilder$$anon$1$$anon$2.insert(DeltaTableV2.scala:432)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.SupportsV1Write.writeWithV1(V1FallbackWriters.scala:79)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.SupportsV1Write.writeWithV1$(V1FallbackWriters.scala:78)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExecV1.writeWithV1(V1FallbackWriters.scala:34)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V1FallbackWriters.run(V1FallbackWriters.scala:66)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V1FallbackWriters.run$(V1FallbackWriters.scala:65)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExecV1.run(V1FallbackWriters.scala:34)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n",
      "\tat sun.reflect.GeneratedMethodAccessor54.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/08/05 18:27:41 ERROR Utils: Uncaught exception in thread stop-spark-context3]\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient.stop(StandaloneAppClient.scala:288)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.org$apache$spark$scheduler$cluster$StandaloneSchedulerBackend$$stop(StandaloneSchedulerBackend.scala:275)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.stop(StandaloneSchedulerBackend.scala:142)\n",
      "\tat org.apache.spark.scheduler.SchedulerBackend.stop(SchedulerBackend.scala:33)\n",
      "\tat org.apache.spark.scheduler.SchedulerBackend.stop$(SchedulerBackend.scala:33)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.stop(CoarseGrainedSchedulerBackend.scala:54)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$stop$2(TaskSchedulerImpl.scala:992)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:992)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$4(DAGScheduler.scala:2976)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2976)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2216)\n",
      "\tat org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:2203)\n",
      "Caused by: org.apache.spark.SparkException: Could not find AppClient.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:178)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:554)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:558)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:72)\n",
      "\t... 17 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o460.sql.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: KILLED\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3573)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3573)\n\tat org.apache.spark.sql.delta.Snapshot.protocolMetadataAndICTReconstruction(Snapshot.scala:309)\n\tat org.apache.spark.sql.delta.Snapshot._reconstructedProtocolMetadataAndICT$lzycompute(Snapshot.scala:203)\n\tat org.apache.spark.sql.delta.Snapshot._reconstructedProtocolMetadataAndICT(Snapshot.scala:196)\n\tat org.apache.spark.sql.delta.Snapshot.metadata(Snapshot.scala:278)\n\tat org.apache.spark.sql.delta.managedcommit.CommitOwnerProvider$.getTableCommitOwner(CommitOwnerClient.scala:212)\n\tat org.apache.spark.sql.delta.Snapshot.initializeTableCommitOwner(Snapshot.scala:237)\n\tat org.apache.spark.sql.delta.Snapshot.<init>(Snapshot.scala:235)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$createSnapshot$2(SnapshotManagement.scala:634)\n\tat org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment(SnapshotManagement.scala:796)\n\tat org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment$(SnapshotManagement.scala:782)\n\tat org.apache.spark.sql.delta.DeltaLog.createSnapshotFromGivenOrEquivalentLogSegment(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.createSnapshot(SnapshotManagement.scala:627)\n\tat org.apache.spark.sql.delta.SnapshotManagement.createSnapshot$(SnapshotManagement.scala:618)\n\tat org.apache.spark.sql.delta.DeltaLog.createSnapshot(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotForLogSegmentInternal$1(SnapshotManagement.scala:1043)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotForLogSegmentInternal(SnapshotManagement.scala:1036)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotForLogSegmentInternal$(SnapshotManagement.scala:1031)\n\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotForLogSegmentInternal(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getUpdatedSnapshot(SnapshotManagement.scala:1012)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getUpdatedSnapshot$(SnapshotManagement.scala:1003)\n\tat org.apache.spark.sql.delta.DeltaLog.getUpdatedSnapshot(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$2(SnapshotManagement.scala:583)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:573)\n\tat org.apache.spark.sql.delta.SnapshotManagement.withSnapshotLockInterruptibly(SnapshotManagement.scala:78)\n\tat org.apache.spark.sql.delta.SnapshotManagement.withSnapshotLockInterruptibly$(SnapshotManagement.scala:75)\n\tat org.apache.spark.sql.delta.DeltaLog.withSnapshotLockInterruptibly(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:573)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:572)\n\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:69)\n\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:80)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:853)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:848)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:651)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:136)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:651)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:135)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:125)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:115)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:651)\n\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:847)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:866)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:865)\n\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:875)\n\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:751)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:92)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:367)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:92)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:90)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$initialSnapshot$4(DeltaTableV2.scala:145)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:145)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:367)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:144)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:124)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:168)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:168)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:166)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.schema(DeltaTableV2.scala:173)\n\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:65)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.columns(DeltaTableV2.scala:58)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation$.create(DataSourceV2Relation.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$createRelation$1(Analyzer.scala:1249)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.createRelation(Analyzer.scala:1221)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$3(Analyzer.scala:1272)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1270)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1262)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1126)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1090)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1214)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1214)\n\tat org.apache.spark.sql.catalyst.plans.logical.InsertIntoStatement.mapChildren(statements.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1090)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1049)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:226)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat sun.reflect.GeneratedMethodAccessor54.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o460.sql.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: KILLED\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3573)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3573)\n\tat org.apache.spark.sql.delta.Snapshot.protocolMetadataAndICTReconstruction(Snapshot.scala:309)\n\tat org.apache.spark.sql.delta.Snapshot._reconstructedProtocolMetadataAndICT$lzycompute(Snapshot.scala:203)\n\tat org.apache.spark.sql.delta.Snapshot._reconstructedProtocolMetadataAndICT(Snapshot.scala:196)\n\tat org.apache.spark.sql.delta.Snapshot.metadata(Snapshot.scala:278)\n\tat org.apache.spark.sql.delta.managedcommit.CommitOwnerProvider$.getTableCommitOwner(CommitOwnerClient.scala:212)\n\tat org.apache.spark.sql.delta.Snapshot.initializeTableCommitOwner(Snapshot.scala:237)\n\tat org.apache.spark.sql.delta.Snapshot.<init>(Snapshot.scala:235)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$createSnapshot$2(SnapshotManagement.scala:634)\n\tat org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment(SnapshotManagement.scala:796)\n\tat org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment$(SnapshotManagement.scala:782)\n\tat org.apache.spark.sql.delta.DeltaLog.createSnapshotFromGivenOrEquivalentLogSegment(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.createSnapshot(SnapshotManagement.scala:627)\n\tat org.apache.spark.sql.delta.SnapshotManagement.createSnapshot$(SnapshotManagement.scala:618)\n\tat org.apache.spark.sql.delta.DeltaLog.createSnapshot(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotForLogSegmentInternal$1(SnapshotManagement.scala:1043)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotForLogSegmentInternal(SnapshotManagement.scala:1036)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotForLogSegmentInternal$(SnapshotManagement.scala:1031)\n\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotForLogSegmentInternal(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getUpdatedSnapshot(SnapshotManagement.scala:1012)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getUpdatedSnapshot$(SnapshotManagement.scala:1003)\n\tat org.apache.spark.sql.delta.DeltaLog.getUpdatedSnapshot(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$2(SnapshotManagement.scala:583)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:573)\n\tat org.apache.spark.sql.delta.SnapshotManagement.withSnapshotLockInterruptibly(SnapshotManagement.scala:78)\n\tat org.apache.spark.sql.delta.SnapshotManagement.withSnapshotLockInterruptibly$(SnapshotManagement.scala:75)\n\tat org.apache.spark.sql.delta.DeltaLog.withSnapshotLockInterruptibly(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:573)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:572)\n\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:69)\n\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:80)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:853)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:848)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:651)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:136)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:651)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:135)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:125)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:115)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:651)\n\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:847)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:866)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:865)\n\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:875)\n\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:751)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:92)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:367)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:92)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:90)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$initialSnapshot$4(DeltaTableV2.scala:145)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:145)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:367)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:144)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:124)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:168)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:168)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:166)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.schema(DeltaTableV2.scala:173)\n\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:65)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.columns(DeltaTableV2.scala:58)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation$.create(DataSourceV2Relation.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$createRelation$1(Analyzer.scala:1249)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.createRelation(Analyzer.scala:1221)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$3(Analyzer.scala:1272)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1270)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1262)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1126)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1090)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1214)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1214)\n\tat org.apache.spark.sql.catalyst.plans.logical.InsertIntoStatement.mapChildren(statements.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1090)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1049)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:226)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat sun.reflect.GeneratedMethodAccessor54.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting data for table rawdata_2\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o460.sql.\n: org.sparkproject.guava.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:750)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2263)\n\tat org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)\n\tat org.sparkproject.guava.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getCachedPlan(SessionCatalog.scala:209)\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(DataSourceStrategy.scala:248)\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:281)\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:278)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:278)\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:242)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:226)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat sun.reflect.GeneratedMethodAccessor54.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:750)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1659)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat.buildReader(JsonFileFormat.scala:99)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:346)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\n\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:242)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:445)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3573)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3573)\n\tat org.apache.spark.sql.delta.Snapshot.protocolMetadataAndICTReconstruction(Snapshot.scala:309)\n\tat org.apache.spark.sql.delta.Snapshot._reconstructedProtocolMetadataAndICT$lzycompute(Snapshot.scala:203)\n\tat org.apache.spark.sql.delta.Snapshot._reconstructedProtocolMetadataAndICT(Snapshot.scala:196)\n\tat org.apache.spark.sql.delta.Snapshot.metadata(Snapshot.scala:278)\n\tat org.apache.spark.sql.delta.managedcommit.CommitOwnerProvider$.getTableCommitOwner(CommitOwnerClient.scala:212)\n\tat org.apache.spark.sql.delta.Snapshot.initializeTableCommitOwner(Snapshot.scala:237)\n\tat org.apache.spark.sql.delta.Snapshot.<init>(Snapshot.scala:235)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$createSnapshot$2(SnapshotManagement.scala:634)\n\tat org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment(SnapshotManagement.scala:796)\n\tat org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment$(SnapshotManagement.scala:782)\n\tat org.apache.spark.sql.delta.DeltaLog.createSnapshotFromGivenOrEquivalentLogSegment(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.createSnapshot(SnapshotManagement.scala:627)\n\tat org.apache.spark.sql.delta.SnapshotManagement.createSnapshot$(SnapshotManagement.scala:618)\n\tat org.apache.spark.sql.delta.DeltaLog.createSnapshot(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotForLogSegmentInternal$1(SnapshotManagement.scala:1043)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotForLogSegmentInternal(SnapshotManagement.scala:1036)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotForLogSegmentInternal$(SnapshotManagement.scala:1031)\n\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotForLogSegmentInternal(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getUpdatedSnapshot(SnapshotManagement.scala:1012)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getUpdatedSnapshot$(SnapshotManagement.scala:1003)\n\tat org.apache.spark.sql.delta.DeltaLog.getUpdatedSnapshot(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$2(SnapshotManagement.scala:583)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:573)\n\tat org.apache.spark.sql.delta.SnapshotManagement.withSnapshotLockInterruptibly(SnapshotManagement.scala:78)\n\tat org.apache.spark.sql.delta.SnapshotManagement.withSnapshotLockInterruptibly$(SnapshotManagement.scala:75)\n\tat org.apache.spark.sql.delta.DeltaLog.withSnapshotLockInterruptibly(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:573)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:572)\n\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:69)\n\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:80)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:853)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:848)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:651)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:136)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:651)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:135)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:125)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:115)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:651)\n\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:847)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:866)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:865)\n\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:875)\n\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:751)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:92)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:367)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:92)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:90)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$initialSnapshot$4(DeltaTableV2.scala:145)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:145)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:367)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:144)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:124)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation$lzycompute(DeltaTableV2.scala:236)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:234)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$5(DeltaDataSource.scala:250)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:49)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:209)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.$anonfun$readDataSourceTable$1(DataSourceStrategy.scala:260)\n\tat org.sparkproject.guava.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n\tat org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\tat org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\t... 68 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o460.sql.\n: org.sparkproject.guava.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:750)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2263)\n\tat org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)\n\tat org.sparkproject.guava.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getCachedPlan(SessionCatalog.scala:209)\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(DataSourceStrategy.scala:248)\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:281)\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply$2.applyOrElse(DataSourceStrategy.scala:278)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:278)\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:242)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:226)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat sun.reflect.GeneratedMethodAccessor54.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:750)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1659)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat.buildReader(JsonFileFormat.scala:99)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:346)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\n\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:242)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:445)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3573)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3573)\n\tat org.apache.spark.sql.delta.Snapshot.protocolMetadataAndICTReconstruction(Snapshot.scala:309)\n\tat org.apache.spark.sql.delta.Snapshot._reconstructedProtocolMetadataAndICT$lzycompute(Snapshot.scala:203)\n\tat org.apache.spark.sql.delta.Snapshot._reconstructedProtocolMetadataAndICT(Snapshot.scala:196)\n\tat org.apache.spark.sql.delta.Snapshot.metadata(Snapshot.scala:278)\n\tat org.apache.spark.sql.delta.managedcommit.CommitOwnerProvider$.getTableCommitOwner(CommitOwnerClient.scala:212)\n\tat org.apache.spark.sql.delta.Snapshot.initializeTableCommitOwner(Snapshot.scala:237)\n\tat org.apache.spark.sql.delta.Snapshot.<init>(Snapshot.scala:235)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$createSnapshot$2(SnapshotManagement.scala:634)\n\tat org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment(SnapshotManagement.scala:796)\n\tat org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment$(SnapshotManagement.scala:782)\n\tat org.apache.spark.sql.delta.DeltaLog.createSnapshotFromGivenOrEquivalentLogSegment(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.createSnapshot(SnapshotManagement.scala:627)\n\tat org.apache.spark.sql.delta.SnapshotManagement.createSnapshot$(SnapshotManagement.scala:618)\n\tat org.apache.spark.sql.delta.DeltaLog.createSnapshot(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotForLogSegmentInternal$1(SnapshotManagement.scala:1043)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotForLogSegmentInternal(SnapshotManagement.scala:1036)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotForLogSegmentInternal$(SnapshotManagement.scala:1031)\n\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotForLogSegmentInternal(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getUpdatedSnapshot(SnapshotManagement.scala:1012)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getUpdatedSnapshot$(SnapshotManagement.scala:1003)\n\tat org.apache.spark.sql.delta.DeltaLog.getUpdatedSnapshot(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$2(SnapshotManagement.scala:583)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:573)\n\tat org.apache.spark.sql.delta.SnapshotManagement.withSnapshotLockInterruptibly(SnapshotManagement.scala:78)\n\tat org.apache.spark.sql.delta.SnapshotManagement.withSnapshotLockInterruptibly$(SnapshotManagement.scala:75)\n\tat org.apache.spark.sql.delta.DeltaLog.withSnapshotLockInterruptibly(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:573)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:572)\n\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:69)\n\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:80)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:853)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:848)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:651)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:136)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:651)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:135)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:125)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:115)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:651)\n\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:847)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:866)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:865)\n\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:875)\n\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:751)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:92)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:367)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:92)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:90)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$initialSnapshot$4(DeltaTableV2.scala:145)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:145)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:367)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:144)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:124)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation$lzycompute(DeltaTableV2.scala:236)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:234)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$5(DeltaDataSource.scala:250)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:49)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:209)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.execution.datasources.FindDataSourceTable.$anonfun$readDataSourceTable$1(DataSourceStrategy.scala:260)\n\tat org.sparkproject.guava.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n\tat org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\tat org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\t... 68 more\n"
     ]
    }
   ],
   "source": [
    "table_pairs = [\n",
    "    ('rawdata_8', 'raw_data.raw_data_8'),\n",
    "    ('rawdata_4', 'raw_data.raw_data_4'),\n",
    "    ('rawdata_2', 'raw_data.raw_data_2')\n",
    "]\n",
    "\n",
    "for target_table, source_table in table_pairs:\n",
    "    insert_sql = f\"\"\"\n",
    "    INSERT INTO {target_table}\n",
    "    SELECT * FROM {source_table}\n",
    "    \"\"\"\n",
    "    print(f\"Inserting data for table {target_table}\")\n",
    "    %time spark.sql(insert_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f147dd-d099-4a8d-8b11-220dbf6d4903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "table_paths = [\n",
    "    \"hdfs://192.168.0.144:9000/datalake/raw_data/rawdata_8\",\n",
    "    \"hdfs://192.168.0.144:9000/datalake/raw_data/rawdata_4\",\n",
    "    \"hdfs://192.168.0.144:9000/datalake/raw_data/rawdata_2\"\n",
    "]\n",
    "\n",
    "for path in table_paths:\n",
    "    delta_table_obj = DeltaTable.forPath(spark, path)\n",
    "    \n",
    "    table_name = path.split(\"/\")[-1]\n",
    "    print(f\"Optimizing and compacting table: {table_name}\")\n",
    "    \n",
    "    %time delta_table_obj.optimize().executeCompaction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16f91f36-35d2-453d-9e1b-7e27436b465f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vaccuum table: rawdata_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 57 files and directories in a total of 1 directories.\n",
      "CPU times: user 41.8 ms, sys: 104 ms, total: 146 ms\n",
      "Wall time: 37.2 s\n"
     ]
    }
   ],
   "source": [
    "def vacuum_delta_table(table_path):\n",
    "    spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "    delta_table_obj = DeltaTable.forPath(spark, table_path)\n",
    "    delta_table_obj.vacuum(0)\n",
    "\n",
    "for path in table_paths:\n",
    "    table_name = path.split(\"/\")[-1]\n",
    "    print(f\"Vaccuum table: {table_name}\")\n",
    "    %time vacuum_delta_table(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9705fc4a-bf65-441e-b7ea-bc1367fc07b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o460.sql.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:750)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1659)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat.buildReader(JsonFileFormat.scala:99)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:346)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\n\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:242)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:445)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3573)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3573)\n\tat org.apache.spark.sql.delta.Snapshot.protocolMetadataAndICTReconstruction(Snapshot.scala:309)\n\tat org.apache.spark.sql.delta.Snapshot._reconstructedProtocolMetadataAndICT$lzycompute(Snapshot.scala:203)\n\tat org.apache.spark.sql.delta.Snapshot._reconstructedProtocolMetadataAndICT(Snapshot.scala:196)\n\tat org.apache.spark.sql.delta.Snapshot.metadata(Snapshot.scala:278)\n\tat org.apache.spark.sql.delta.managedcommit.CommitOwnerProvider$.getTableCommitOwner(CommitOwnerClient.scala:212)\n\tat org.apache.spark.sql.delta.Snapshot.initializeTableCommitOwner(Snapshot.scala:237)\n\tat org.apache.spark.sql.delta.Snapshot.<init>(Snapshot.scala:235)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$createSnapshot$2(SnapshotManagement.scala:634)\n\tat org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment(SnapshotManagement.scala:796)\n\tat org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment$(SnapshotManagement.scala:782)\n\tat org.apache.spark.sql.delta.DeltaLog.createSnapshotFromGivenOrEquivalentLogSegment(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.createSnapshot(SnapshotManagement.scala:627)\n\tat org.apache.spark.sql.delta.SnapshotManagement.createSnapshot$(SnapshotManagement.scala:618)\n\tat org.apache.spark.sql.delta.DeltaLog.createSnapshot(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotForLogSegmentInternal$1(SnapshotManagement.scala:1043)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotForLogSegmentInternal(SnapshotManagement.scala:1036)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotForLogSegmentInternal$(SnapshotManagement.scala:1031)\n\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotForLogSegmentInternal(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getUpdatedSnapshot(SnapshotManagement.scala:1012)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getUpdatedSnapshot$(SnapshotManagement.scala:1003)\n\tat org.apache.spark.sql.delta.DeltaLog.getUpdatedSnapshot(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$2(SnapshotManagement.scala:583)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:573)\n\tat org.apache.spark.sql.delta.SnapshotManagement.withSnapshotLockInterruptibly(SnapshotManagement.scala:78)\n\tat org.apache.spark.sql.delta.SnapshotManagement.withSnapshotLockInterruptibly$(SnapshotManagement.scala:75)\n\tat org.apache.spark.sql.delta.DeltaLog.withSnapshotLockInterruptibly(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:573)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:572)\n\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:69)\n\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:80)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:853)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:848)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:651)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:136)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:651)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:135)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:125)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:115)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:651)\n\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:847)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:866)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:865)\n\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:875)\n\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:751)\n\tat org.apache.spark.sql.delta.commands.DescribeDeltaDetailCommand.run(DescribeDeltaDetailsCommand.scala:93)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat sun.reflect.GeneratedMethodAccessor54.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdisplay_table_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraw_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrawdata_2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrawdata_4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrawdata_8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 17\u001b[0m, in \u001b[0;36mdisplay_table_properties\u001b[0;34m(spark, database, tables)\u001b[0m\n\u001b[1;32m     14\u001b[0m table_details \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table_name \u001b[38;5;129;01min\u001b[39;00m tables:\n\u001b[0;32m---> 17\u001b[0m     describe_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDESCRIBE DETAIL \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtable_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     describe_detail \u001b[38;5;241m=\u001b[39m describe_df\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39masDict()\n\u001b[1;32m     19\u001b[0m     hdfs_location \u001b[38;5;241m=\u001b[39m describe_detail[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o460.sql.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:750)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1659)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1644)\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat.buildReader(JsonFileFormat.scala:99)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:346)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:575)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\n\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:242)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:445)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3573)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3573)\n\tat org.apache.spark.sql.delta.Snapshot.protocolMetadataAndICTReconstruction(Snapshot.scala:309)\n\tat org.apache.spark.sql.delta.Snapshot._reconstructedProtocolMetadataAndICT$lzycompute(Snapshot.scala:203)\n\tat org.apache.spark.sql.delta.Snapshot._reconstructedProtocolMetadataAndICT(Snapshot.scala:196)\n\tat org.apache.spark.sql.delta.Snapshot.metadata(Snapshot.scala:278)\n\tat org.apache.spark.sql.delta.managedcommit.CommitOwnerProvider$.getTableCommitOwner(CommitOwnerClient.scala:212)\n\tat org.apache.spark.sql.delta.Snapshot.initializeTableCommitOwner(Snapshot.scala:237)\n\tat org.apache.spark.sql.delta.Snapshot.<init>(Snapshot.scala:235)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$createSnapshot$2(SnapshotManagement.scala:634)\n\tat org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment(SnapshotManagement.scala:796)\n\tat org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment$(SnapshotManagement.scala:782)\n\tat org.apache.spark.sql.delta.DeltaLog.createSnapshotFromGivenOrEquivalentLogSegment(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.createSnapshot(SnapshotManagement.scala:627)\n\tat org.apache.spark.sql.delta.SnapshotManagement.createSnapshot$(SnapshotManagement.scala:618)\n\tat org.apache.spark.sql.delta.DeltaLog.createSnapshot(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotForLogSegmentInternal$1(SnapshotManagement.scala:1043)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotForLogSegmentInternal(SnapshotManagement.scala:1036)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotForLogSegmentInternal$(SnapshotManagement.scala:1031)\n\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotForLogSegmentInternal(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getUpdatedSnapshot(SnapshotManagement.scala:1012)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getUpdatedSnapshot$(SnapshotManagement.scala:1003)\n\tat org.apache.spark.sql.delta.DeltaLog.getUpdatedSnapshot(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$2(SnapshotManagement.scala:583)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:573)\n\tat org.apache.spark.sql.delta.SnapshotManagement.withSnapshotLockInterruptibly(SnapshotManagement.scala:78)\n\tat org.apache.spark.sql.delta.SnapshotManagement.withSnapshotLockInterruptibly$(SnapshotManagement.scala:75)\n\tat org.apache.spark.sql.delta.DeltaLog.withSnapshotLockInterruptibly(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:573)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:572)\n\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:69)\n\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:80)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:853)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:848)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:651)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:136)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:651)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:135)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:125)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:115)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:651)\n\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:847)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:866)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:865)\n\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:875)\n\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:751)\n\tat org.apache.spark.sql.delta.commands.DescribeDeltaDetailCommand.run(DescribeDeltaDetailsCommand.scala:93)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat sun.reflect.GeneratedMethodAccessor54.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "display_table_properties(spark, \"raw_data\", ['rawdata_2', 'rawdata_4', 'rawdata_8'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0a3fa0-492c-431b-92f2-673bb4af2851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from py4j.java_gateway import java_import\n",
    "\n",
    "java_import(spark._jvm, \"org.apache.hadoop.fs.FileSystem\")\n",
    "java_import(spark._jvm, \"org.apache.hadoop.fs.Path\")\n",
    "\n",
    "def list_hdfs_directory(hdfs_path, level=0):\n",
    "    hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "    fs = spark._jvm.FileSystem.get(hadoop_conf)\n",
    "    path = spark._jvm.Path(hdfs_path)\n",
    "    \n",
    "    if level == 0:\n",
    "        print(f\"Listing contents of: {hdfs_path}\")\n",
    "    \n",
    "    status = fs.listStatus(path)\n",
    "    \n",
    "    for file_status in status:\n",
    "        file_path = file_status.getPath()\n",
    "        is_dir = file_status.isDirectory()\n",
    "        size = file_status.getLen() if not is_dir else 0\n",
    "        indent = ' ' * (level * 2)\n",
    "        if is_dir:\n",
    "            print(f\"{indent}- {file_path.getName()}/\")\n",
    "            list_hdfs_directory(file_path.toString(), level + 1)\n",
    "        else:\n",
    "            size_mb = size / (1024 * 1024)\n",
    "            print(f\"{indent}- {file_path.getName()} ({size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1c46e2e-7af7-470d-b80c-4df62bffde3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "\n",
    "num_executions = 2\n",
    "\n",
    "\n",
    "def measure_execution_time(query):\n",
    "    spark.catalog.clearCache()\n",
    "    start_time = time.time()\n",
    "    spark.sql(query).show() \n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "def get_average_execution_times(query_template, num_executions, tables):\n",
    "    execution_times = {table: [] for table in tables}\n",
    "    for table in tables:\n",
    "        for _ in range(num_executions):\n",
    "            query = query_template.format(table=table)\n",
    "            try:\n",
    "                execution_time = measure_execution_time(query)\n",
    "                print(f\"{table}: {execution_time} seconds\")\n",
    "                execution_times[table].append(execution_time)\n",
    "            except Exception as e:\n",
    "                print(f\"Error for table {table}: {e}\")\n",
    "    average_times = {table: sum(times) / num_executions for table, times in execution_times.items()}\n",
    "    return average_times\n",
    "\n",
    "query1_template = \"\"\"\n",
    "SELECT\n",
    "    raw_data.product_dim.product_name,\n",
    "    raw_data.location_dim.city,\n",
    "    raw_data.location_dim.state,\n",
    "    raw_data.location_dim.country,\n",
    "    COUNT({table}.event_id) AS event_count\n",
    "FROM\n",
    "    {table}\n",
    "JOIN\n",
    "    raw_data.product_dim ON {table}.product_id = raw_data.product_dim.product_id\n",
    "JOIN\n",
    "    raw_data.location_dim ON {table}.location_id = raw_data.location_dim.location_id\n",
    "WHERE\n",
    "    {table}.event_id BETWEEN 459999 AND 999999  \n",
    "    AND {table}.actor_id IN (5001, 5002, 5003)  \n",
    "GROUP BY\n",
    "    raw_data.product_dim.product_name,\n",
    "    raw_data.location_dim.city,\n",
    "    raw_data.location_dim.state,\n",
    "    raw_data.location_dim.country;\n",
    "\"\"\"\n",
    "\n",
    "query2_template = \"\"\"\n",
    "SELECT\n",
    "    raw_data.product_dim.product_name,\n",
    "    raw_data.location_dim.city,\n",
    "    raw_data.location_dim.state,\n",
    "    raw_data.location_dim.country,\n",
    "    avg({table}.value) AS total_value\n",
    "FROM\n",
    "    {table}\n",
    "JOIN\n",
    "    raw_data.product_dim ON {table}.product_id = raw_data.product_dim.product_id\n",
    "JOIN\n",
    "    raw_data.location_dim ON {table}.location_id = raw_data.location_dim.location_id\n",
    "WHERE\n",
    "    {table}.event_id BETWEEN 459999 AND 999999  \n",
    "    AND {table}.actor_id IN (5001, 5002, 5003)  \n",
    "GROUP BY\n",
    "    raw_data.product_dim.product_name,\n",
    "    raw_data.location_dim.city,\n",
    "    raw_data.location_dim.state,\n",
    "    raw_data.location_dim.country\n",
    "ORDER BY\n",
    "    total_value DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "674498db-96ad-41fd-8d60-13c7f801a020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----+-------+-----------+\n",
      "|product_name|        city|state|country|event_count|\n",
      "+------------+------------+-----+-------+-----------+\n",
      "|   Product G| San Antonio|   TX|    USA|     129012|\n",
      "|   Product D|     Houston|   TX|    USA|     129594|\n",
      "|   Product H|   San Diego|   CA|    USA|     129806|\n",
      "|   Product B| Los Angeles|   CA|    USA|     129185|\n",
      "|   Product F|Philadelphia|   PA|    USA|     129374|\n",
      "|   Product C|     Chicago|   IL|    USA|     129489|\n",
      "|   Product J|    San Jose|   CA|    USA|     129137|\n",
      "|   Product E|     Phoenix|   AZ|    USA|     129858|\n",
      "|   Product I|      Dallas|   TX|    USA|     129584|\n",
      "|   Product A|    New York|   NY|    USA|     129076|\n",
      "+------------+------------+-----+-------+-----------+\n",
      "\n",
      "raw_data.raw_data_8: 2633.015395641327 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----+-------+-----------+\n",
      "|product_name|        city|state|country|event_count|\n",
      "+------------+------------+-----+-------+-----------+\n",
      "|   Product G| San Antonio|   TX|    USA|     129012|\n",
      "|   Product D|     Houston|   TX|    USA|     129594|\n",
      "|   Product H|   San Diego|   CA|    USA|     129806|\n",
      "|   Product B| Los Angeles|   CA|    USA|     129185|\n",
      "|   Product F|Philadelphia|   PA|    USA|     129374|\n",
      "|   Product C|     Chicago|   IL|    USA|     129489|\n",
      "|   Product J|    San Jose|   CA|    USA|     129137|\n",
      "|   Product E|     Phoenix|   AZ|    USA|     129858|\n",
      "|   Product I|      Dallas|   TX|    USA|     129584|\n",
      "|   Product A|    New York|   NY|    USA|     129076|\n",
      "+------------+------------+-----+-------+-----------+\n",
      "\n",
      "raw_data.raw_data_8: 3155.9342246055603 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----+-------+-----------+\n",
      "|product_name|        city|state|country|event_count|\n",
      "+------------+------------+-----+-------+-----------+\n",
      "|   Product G| San Antonio|   TX|    USA|      64622|\n",
      "|   Product D|     Houston|   TX|    USA|      64944|\n",
      "|   Product H|   San Diego|   CA|    USA|      64856|\n",
      "|   Product B| Los Angeles|   CA|    USA|      64775|\n",
      "|   Product F|Philadelphia|   PA|    USA|      64727|\n",
      "|   Product C|     Chicago|   IL|    USA|      64872|\n",
      "|   Product J|    San Jose|   CA|    USA|      64522|\n",
      "|   Product E|     Phoenix|   AZ|    USA|      64419|\n",
      "|   Product I|      Dallas|   TX|    USA|      64561|\n",
      "|   Product A|    New York|   NY|    USA|      65019|\n",
      "+------------+------------+-----+-------+-----------+\n",
      "\n",
      "raw_data.raw_data_4: 1574.626448392868 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.===>        (543 + 15) / 647]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lh/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/lh/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m tables \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_data.raw_data_8\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_data.raw_data_4\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_data.raw_data_2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m ]\n\u001b[0;32m----> 7\u001b[0m average_times_query1_2gb \u001b[38;5;241m=\u001b[39m \u001b[43mget_average_execution_times\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery1_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_executions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m tables \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_data.rawdata_8\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_data.rawdata_4\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_data.rawdata_2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m ]\n\u001b[1;32m     15\u001b[0m average_times_query1_1gb \u001b[38;5;241m=\u001b[39m get_average_execution_times(query1_template, num_executions, tables)\n",
      "Cell \u001b[0;32mIn[20], line 23\u001b[0m, in \u001b[0;36mget_average_execution_times\u001b[0;34m(query_template, num_executions, tables)\u001b[0m\n\u001b[1;32m     21\u001b[0m query \u001b[38;5;241m=\u001b[39m query_template\u001b[38;5;241m.\u001b[39mformat(table\u001b[38;5;241m=\u001b[39mtable)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     execution_time \u001b[38;5;241m=\u001b[39m \u001b[43mmeasure_execution_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecution_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m     execution_times[table]\u001b[38;5;241m.\u001b[39mappend(execution_time)\n",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m, in \u001b[0;36mmeasure_execution_time\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     11\u001b[0m spark\u001b[38;5;241m.\u001b[39mcatalog\u001b[38;5;241m.\u001b[39mclearCache()\n\u001b[1;32m     12\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 13\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     14\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tables = [\n",
    "    'raw_data.raw_data_8',\n",
    "    'raw_data.raw_data_4',\n",
    "    'raw_data.raw_data_2'\n",
    "]\n",
    "\n",
    "average_times_query1_2gb = get_average_execution_times(query1_template, num_executions, tables)\n",
    "\n",
    "tables = [\n",
    "    'raw_data.rawdata_8',\n",
    "    'raw_data.rawdata_4',\n",
    "    'raw_data.rawdata_2'\n",
    "]\n",
    "\n",
    "average_times_query1_1gb = get_average_execution_times(query1_template, num_executions, tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3c5254-d7cc-4dba-bc62-a60bb107ab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(average_times_query1_2gb)\n",
    "print(average_times_query1_1gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f136b2-c806-40ef-aab0-5a312cde346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_comparison(before_times, after_times, query_num):\n",
    "    plt.rcParams.update({'font.size': 14})  \n",
    "    plt.rcParams.update({'axes.titlesize': 14}) \n",
    "    plt.rcParams.update({'axes.labelsize': 14})  \n",
    "    plt.rcParams.update({'xtick.labelsize': 14})  \n",
    "    plt.rcParams.update({'ytick.labelsize': 14}) \n",
    "    plt.rcParams.update({'legend.fontsize': 14})  \n",
    "    plt.rcParams.update({'font.family': 'serif'})\n",
    "    plt.rcParams.update({'font.serif': 'DejaVu Serif'})\n",
    "\n",
    "    df_before = pd.DataFrame(list(before_times.items()), columns=[\"Table\", \"Average Execution Time Before Partitioning\"])\n",
    "    df_after = pd.DataFrame(list(after_times.items()), columns=[\"Table\", \"Average Execution Time After Partitioning\"])\n",
    "    df = df_before.merge(df_after, on=\"Table\")\n",
    "\n",
    "    ax = df.plot(kind=\"bar\", x=\"Table\", figsize=(12, 6))\n",
    "    plt.ylabel('Average Execution Time (seconds)')\n",
    "    plt.xlabel('')\n",
    "    plt.title(f'Average Query Execution Time for Each Table (Query {query_num})')\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    plt.ylim(0, 110)\n",
    "    legend = ax.legend(loc='upper left', bbox_to_anchor=(0, 1), prop={'size': 12})  \n",
    "\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.annotate(f'{height:.2f}', \n",
    "                    (p.get_x() + p.get_width() / 2., height * 1.01), \n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'raw_table_performance_{query_num}.png', dpi=500)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea1afd7-2c97-4179-8035-85de963a594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(average_times_query1_2gb, average_times_query1_1gb, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22c93bc1-9e11-450f-8f99-8dac81958e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "\n",
    "num_executions = 1\n",
    "\n",
    "\n",
    "def measure_execution_time(query):\n",
    "    spark.catalog.clearCache()\n",
    "    start_time = time.time()\n",
    "    spark.sql(query).show() \n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "def get_average_execution_times(query_template, num_executions, tables):\n",
    "    execution_times = {table: [] for table in tables}\n",
    "    for table in tables:\n",
    "        for _ in range(num_executions):\n",
    "            query = query_template.format(table=table)\n",
    "            try:\n",
    "                execution_time = measure_execution_time(query)\n",
    "                print(f\"{table}: {execution_time} seconds\")\n",
    "                execution_times[table].append(execution_time)\n",
    "            except Exception as e:\n",
    "                print(f\"Error for table {table}: {e}\")\n",
    "    average_times = {table: sum(times) / num_executions for table, times in execution_times.items()}\n",
    "    return average_times\n",
    "\n",
    "query1_template = \"\"\"\n",
    "SELECT\n",
    "    raw_data.product_dim.product_name,\n",
    "    raw_data.location_dim.city,\n",
    "    raw_data.location_dim.state,\n",
    "    raw_data.location_dim.country,\n",
    "    COUNT({table}.event_id) AS event_count\n",
    "FROM\n",
    "    {table}\n",
    "JOIN\n",
    "    raw_data.product_dim ON {table}.product_id = raw_data.product_dim.product_id\n",
    "JOIN\n",
    "    raw_data.location_dim ON {table}.location_id = raw_data.location_dim.location_id\n",
    "WHERE\n",
    "    {table}.event_id BETWEEN 459999 AND 999999  \n",
    "    AND {table}.actor_id IN (5001, 5002, 5003)  \n",
    "GROUP BY\n",
    "    raw_data.product_dim.product_name,\n",
    "    raw_data.location_dim.city,\n",
    "    raw_data.location_dim.state,\n",
    "    raw_data.location_dim.country;\n",
    "\"\"\"\n",
    "\n",
    "query2_template = \"\"\"\n",
    "SELECT\n",
    "    raw_data.product_dim.product_name,\n",
    "    raw_data.location_dim.city,\n",
    "    raw_data.location_dim.state,\n",
    "    raw_data.location_dim.country,\n",
    "    avg({table}.value) AS total_value\n",
    "FROM\n",
    "    {table}\n",
    "JOIN\n",
    "    raw_data.product_dim ON {table}.product_id = raw_data.product_dim.product_id\n",
    "JOIN\n",
    "    raw_data.location_dim ON {table}.location_id = raw_data.location_dim.location_id\n",
    "WHERE\n",
    "    {table}.event_id BETWEEN 459999 AND 999999  \n",
    "    AND {table}.actor_id IN (5001, 5002, 5003)  \n",
    "GROUP BY\n",
    "    raw_data.product_dim.product_name,\n",
    "    raw_data.location_dim.city,\n",
    "    raw_data.location_dim.state,\n",
    "    raw_data.location_dim.country\n",
    "ORDER BY\n",
    "    total_value DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14d19423-642f-4d8e-8ed7-dc3afa3f931e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.            (20 + 15) / 1293]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lh/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/lh/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m tables \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_data.raw_data_8\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_data.raw_data_4\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_data.raw_data_2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m ]\n\u001b[0;32m----> 7\u001b[0m average_times_query1_2gb \u001b[38;5;241m=\u001b[39m \u001b[43mget_average_execution_times\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery2_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_executions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m tables \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_data.rawdata_8\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_data.rawdata_4\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_data.rawdata_2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m ]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#average_times_query1_1gb = get_average_execution_times(query2_template, num_executions, tables)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 23\u001b[0m, in \u001b[0;36mget_average_execution_times\u001b[0;34m(query_template, num_executions, tables)\u001b[0m\n\u001b[1;32m     21\u001b[0m query \u001b[38;5;241m=\u001b[39m query_template\u001b[38;5;241m.\u001b[39mformat(table\u001b[38;5;241m=\u001b[39mtable)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     execution_time \u001b[38;5;241m=\u001b[39m \u001b[43mmeasure_execution_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecution_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m     execution_times[table]\u001b[38;5;241m.\u001b[39mappend(execution_time)\n",
      "Cell \u001b[0;32mIn[27], line 13\u001b[0m, in \u001b[0;36mmeasure_execution_time\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     11\u001b[0m spark\u001b[38;5;241m.\u001b[39mcatalog\u001b[38;5;241m.\u001b[39mclearCache()\n\u001b[1;32m     12\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 13\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     14\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tables = [\n",
    "    'raw_data.raw_data_8',\n",
    "    'raw_data.raw_data_4',\n",
    "    'raw_data.raw_data_2'\n",
    "]\n",
    "\n",
    "average_times_query1_2gb = get_average_execution_times(query2_template, num_executions, tables)\n",
    "\n",
    "tables = [\n",
    "    'raw_data.rawdata_8',\n",
    "    'raw_data.rawdata_4',\n",
    "    'raw_data.rawdata_2'\n",
    "]\n",
    "\n",
    "#average_times_query1_1gb = get_average_execution_times(query2_template, num_executions, tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f11ede3-2aa6-4db4-afc1-a38f8f339825",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(average_times_query1_2gb)\n",
    "print(average_times_query1_1gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1917b3bf-a386-45d7-8bea-13be09b6cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_comparison(before_times, after_times, query_num):\n",
    "    plt.rcParams.update({'font.size': 14})  \n",
    "    plt.rcParams.update({'axes.titlesize': 14}) \n",
    "    plt.rcParams.update({'axes.labelsize': 14})  \n",
    "    plt.rcParams.update({'xtick.labelsize': 14})  \n",
    "    plt.rcParams.update({'ytick.labelsize': 14}) \n",
    "    plt.rcParams.update({'legend.fontsize': 14})  \n",
    "    plt.rcParams.update({'font.family': 'serif'})\n",
    "    plt.rcParams.update({'font.serif': 'DejaVu Serif'})\n",
    "\n",
    "    df_before = pd.DataFrame(list(before_times.items()), columns=[\"Table\", \"Average Execution Time Before Partitioning\"])\n",
    "    df_after = pd.DataFrame(list(after_times.items()), columns=[\"Table\", \"Average Execution Time After Partitioning\"])\n",
    "    df = df_before.merge(df_after, on=\"Table\")\n",
    "\n",
    "    ax = df.plot(kind=\"bar\", x=\"Table\", figsize=(12, 6))\n",
    "    plt.ylabel('Average Execution Time (seconds)')\n",
    "    plt.xlabel('')\n",
    "    plt.title(f'Average Query Execution Time for Each Table (Query {query_num})')\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    plt.ylim(0, 110)\n",
    "    legend = ax.legend(loc='upper left', bbox_to_anchor=(0, 1), prop={'size': 12})  \n",
    "\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.annotate(f'{height:.2f}', \n",
    "                    (p.get_x() + p.get_width() / 2., height * 1.01), \n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'raw_table_performance_{query_num}.png', dpi=500)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dacfa3-ce84-4b95-897a-196ad0b58388",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(average_times_query1_2gb, average_times_query1_1gb, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dedaaa61-5e5b-4455-bba7-3f8d6a6b4d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:======================================================>(328 + 2) / 330]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|       total_value|\n",
      "+------------------+\n",
      "|0.4999880171256666|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    raw_data.product_dim.product_name,\n",
    "    raw_data.location_dim.city,\n",
    "    raw_data.location_dim.state,\n",
    "    raw_data.location_dim.country,\n",
    "    COUNT({table}.event_id) AS event_count\n",
    "FROM\n",
    "    {table}\n",
    "JOIN\n",
    "    raw_data.product_dim ON {table}.product_id = raw_data.product_dim.product_id\n",
    "JOIN\n",
    "    raw_data.location_dim ON {table}.location_id = raw_data.location_dim.location_id\n",
    "WHERE\n",
    "    {table}.event_id BETWEEN 459999 AND 999999  \n",
    "    AND {table}.actor_id IN (5001, 5002, 5003)  \n",
    "GROUP BY\n",
    "    raw_data.product_dim.product_name,\n",
    "    raw_data.location_dim.city,\n",
    "    raw_data.location_dim.state,\n",
    "    raw_data.location_dim.country\n",
    "ORDER BY\n",
    "    total_value DESC;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c466733f-d73e-44a1-8362-936854517fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:=====================================================>(331 + 3) / 334]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|        total_value|\n",
      "+-------------------+\n",
      "|0.49998801712566676|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "\n",
    "    avg(rawdata_2.value) AS total_value\n",
    "FROM\n",
    "    rawdata_2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d654e88-b981-4fcf-ac35-beafbec7e8c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
